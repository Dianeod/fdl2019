{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FDL 2019 - Floods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in any libraries and datasets needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp:.p.import[`geopandas]\n",
    "\\l ml/ml.q \n",
    "\\l ml/init.q\n",
    "\\l ml/fresh/notebooks/graphics.q\n",
    "\n",
    "avs:.p.import[`sklearn.metrics]`:average_precision_score\n",
    "mattab:{flip value flip x}\n",
    "svc:.p.import[`sklearn.svm]`:SVC\n",
    "array:.p.import[`numpy]`:array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets include\n",
    "\n",
    "Max height per day per stream\n",
    "\n",
    "NLCD(imperveous) dataset collected in 2006,2011,2016\n",
    "\n",
    "Flood warned levels from NOAA, based on lat,long\n",
    "\n",
    "stream gage info i.e location,state, codes etc\n",
    "\n",
    "basin attributes at each stream gage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlcd06:(\"S\",(5#\"F\"),\"SFFSFSFFFFSFSSFSSFFFSFFSFF\";enlist \",\") 0:`:data/snap_sampled_imp_nlcd_2006.csv \n",
    "nlcd11:(\"S\",(5#\"F\"),\"SFFSFSFFFFSFSSFSSFFFSFFSFF\";enlist \",\") 0:`:data/snap_sampled_imp_nlcd_2011.csv \n",
    "nlcd16:(\"S\",(5#\"F\"),\"SFFSFSFFFFSFSSFSSFFFSFFSFF\";enlist \",\") 0:`:data/snap_sampled_imp_nlcd_2016.csv \n",
    "\n",
    "warnings:gp[`:read_file][\"data/national_shapefile_obs.shp\"]\n",
    "warnings:.ml.df2tab[warnings]\n",
    "\n",
    "gages:(\"SSSSFFSSIFFFFFFFFSSISSSSFF\";enlist \",\") 0:`:data/usgs_gage_subset.csv\n",
    "\n",
    "basin:(\"S\",242#\"F\";enlist \",\") 0:`:data/gages_with_basin_attr.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linking the stream gages with corresponding rain gages based on site_no and date (per month). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\\l gagesdir/gagesdir  /will change name of loaddir\n",
    "\n",
    "maxht:0!select max height by site_no,date from str\n",
    "\n",
    "maxht[`site_no]:`${$[7=count x;\"0\",x;x]}each maxht[`site_no]\n",
    "\n",
    "maxht:delete from maxht where height<0\n",
    "\n",
    "gages[`site_no]:`${$[7=count x;\"0\",x;x]}each string each gages[`site_no] /pad with 0 if len site=7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\\cd ../../"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the rain data from prism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "site_no long     lat     elv  date       ppt \n",
      "---------------------------------------------\n",
      "1367690 -74.5596 41.1053 1056 2019.01.01 0.94\n",
      "1367690 -74.5596 41.1053 1056 2019.01.02 0   \n",
      "1367690 -74.5596 41.1053 1056 2019.01.03 0   \n",
      "1367690 -74.5596 41.1053 1056 2019.01.04 0   \n",
      "1367690 -74.5596 41.1053 1056 2019.01.05 0.38\n",
      "1367690 -74.5596 41.1053 1056 2019.01.06 0.79\n",
      "1367690 -74.5596 41.1053 1056 2019.01.07 0   \n",
      "1367690 -74.5596 41.1053 1056 2019.01.08 0.02\n",
      "1367690 -74.5596 41.1053 1056 2019.01.09 0.15\n",
      "1367690 -74.5596 41.1053 1056 2019.01.10 0   \n",
      "1367690 -74.5596 41.1053 1056 2019.01.11 0   \n",
      "1367690 -74.5596 41.1053 1056 2019.01.12 0   \n",
      "1367690 -74.5596 41.1053 1056 2019.01.13 0   \n",
      "1367690 -74.5596 41.1053 1056 2019.01.14 0   \n",
      "1367690 -74.5596 41.1053 1056 2019.01.15 0   \n",
      "1367690 -74.5596 41.1053 1056 2019.01.16 0   \n",
      "1367690 -74.5596 41.1053 1056 2019.01.17 0   \n",
      "1367690 -74.5596 41.1053 1056 2019.01.18 0.06\n",
      "1367690 -74.5596 41.1053 1056 2019.01.19 0   \n",
      "1367690 -74.5596 41.1053 1056 2019.01.20 1.02\n",
      "..\n"
     ]
    }
   ],
   "source": [
    "show precipall:raze {flip `site_no`long`lat`elv`date`ppt!flip value each 10_(\"SFFFDF\";enlist \",\")0: \n",
    "    hsym `$\"data/prism/\",string[x]} each key `:data/prism\n",
    "\n",
    "precipall[`site_no]:`${$[7=count x;\"0\",x;x]}each string each precipall[`site_no]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some rain sites are missing the last digits of their id number so have to preprocess to get it to match the stream gages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "`precipall`precipall`precipall`precipall`precipall`precipall`precipall`precip..\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "`precipall`precipall\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rms:asc ds where 12<count each string each ds:exec distinct site_no from gages \n",
    "names:0!select i by site_no from precipall where site_no in `$12#'string each rms\n",
    "\n",
    "{![`precipall;enlist (in;`i;y);0b;(enlist `site_no)!enlist enlist x]}'[rms[til[20],23 24 25];\n",
    "    names[`x][til[19],21 22 23 24]]\n",
    "\n",
    "matchnames:0!select i by lat,long from precipall where i in names[`x][20]\n",
    "{![`precipall;enlist (in;`i;y);0b;(enlist `site_no)!enlist enlist x]}'[rms[21 22];matchnames[`x]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get stats on each rain gage per month,uid (id of raingage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "site_no  date    lat     long     elevation sumpr maxpre avgpre     varpre    \n",
      "------------------------------------------------------------------------------\n",
      "01200000 2009.07 41.6588 -73.5287 456       8.01  2.04   0.2583871  0.2031039 \n",
      "01200000 2009.08 41.6588 -73.5287 456       5.94  1.59   0.1916129  0.1258458 \n",
      "01200000 2009.09 41.6588 -73.5287 456       1.55  0.85   0.05166667 0.02672056\n",
      "01200000 2009.10 41.6588 -73.5287 456       4.51  1.46   0.1454839  0.1004828 \n",
      "01200000 2009.11 41.6588 -73.5287 456       1.52  0.49   0.05066667 0.01351289\n",
      "01200000 2009.12 41.6588 -73.5287 456       5.61  0.93   0.1809677  0.08818939\n",
      "01200000 2010.01 41.6588 -73.5287 456       2.82  1.75   0.09096774 0.0974539 \n",
      "01200000 2010.02 41.6588 -73.5287 456       4.41  2.28   0.1575     0.1941045 \n",
      "01200000 2010.03 41.6588 -73.5287 456       6.5   1.36   0.2096774  0.1715322 \n",
      "01200000 2010.04 41.6588 -73.5287 456       2.17  0.57   0.07233333 0.01860456\n",
      "01200000 2010.05 41.6588 -73.5287 456       1.93  0.69   0.06225806 0.01854651\n",
      "01200000 2010.06 41.6588 -73.5287 456       3.12  0.73   0.104      0.03363733\n",
      "01200000 2010.07 41.6588 -73.5287 456       2.39  0.56   0.07709677 0.02474964\n",
      "01200000 2010.08 41.6588 -73.5287 456       5.81  2.7    0.1874194  0.2565869 \n",
      "01200000 2010.09 41.6588 -73.5287 456       1.95  0.74   0.065      0.03039167\n",
      "01200000 2010.10 41.6588 -73.5287 456       8.96  3.1    0.2890323  0.5052797 \n",
      "01200000 2010.11 41.6588 -73.5287 456       3.19  1.15   0.1063333  0.07340989\n",
      "01200000 2010.12 41.6588 -73.5287 456       4.74  1.8    0.1529032  0.19124   \n",
      "01200000 2011.01 41.6588 -73.5287 456       3.33  0.92   0.1074194  0.03784495\n",
      "01200000 2011.02 41.6588 -73.5287 456       3.45  1.09   0.1232143  0.0569861 \n",
      "..\n"
     ]
    }
   ],
   "source": [
    "show rainmonth:1_0!select first distinct lat,first distinct long,elevation:distinct elv,sumpr:sum ppt,\n",
    "    maxpre:max ppt,avgpre:avg ppt,varpre:var ppt by site_no\n",
    "    ,\"m\"$date from precipall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basin Characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take out columns that depend on the date like 09,10,11. Link it with previous table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols09:where (count each ss[;\"2006\"]each string each cols basin)<>0\n",
    "cols10:where (count each ss[;\"2010\"]each string each cols basin)<>0\n",
    "cols11:where (count each ss[;\"2011\"]each string each cols basin)<>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "site_no  date    lat     long     elevation sumpr maxpre avgpre     varpre   ..\n",
      "-----------------------------------------------------------------------------..\n",
      "01200000 2009.07 41.6588 -73.5287 456       8.01  2.04   0.2583871  0.2031039..\n",
      "01200000 2009.08 41.6588 -73.5287 456       5.94  1.59   0.1916129  0.1258458..\n",
      "01200000 2009.09 41.6588 -73.5287 456       1.55  0.85   0.05166667 0.0267205..\n",
      "01200000 2009.10 41.6588 -73.5287 456       4.51  1.46   0.1454839  0.1004828..\n",
      "01200000 2009.11 41.6588 -73.5287 456       1.52  0.49   0.05066667 0.0135128..\n",
      "01200000 2009.12 41.6588 -73.5287 456       5.61  0.93   0.1809677  0.0881893..\n",
      "01200000 2010.01 41.6588 -73.5287 456       2.82  1.75   0.09096774 0.0974539..\n",
      "01200000 2010.02 41.6588 -73.5287 456       4.41  2.28   0.1575     0.1941045..\n",
      "01200000 2010.03 41.6588 -73.5287 456       6.5   1.36   0.2096774  0.1715322..\n",
      "01200000 2010.04 41.6588 -73.5287 456       2.17  0.57   0.07233333 0.0186045..\n",
      "01200000 2010.05 41.6588 -73.5287 456       1.93  0.69   0.06225806 0.0185465..\n",
      "01200000 2010.06 41.6588 -73.5287 456       3.12  0.73   0.104      0.0336373..\n",
      "01200000 2010.07 41.6588 -73.5287 456       2.39  0.56   0.07709677 0.0247496..\n",
      "01200000 2010.08 41.6588 -73.5287 456       5.81  2.7    0.1874194  0.2565869..\n",
      "01200000 2010.09 41.6588 -73.5287 456       1.95  0.74   0.065      0.0303916..\n",
      "01200000 2010.10 41.6588 -73.5287 456       8.96  3.1    0.2890323  0.5052797..\n",
      "01200000 2010.11 41.6588 -73.5287 456       3.19  1.15   0.1063333  0.0734098..\n",
      "01200000 2010.12 41.6588 -73.5287 456       4.74  1.8    0.1529032  0.19124  ..\n",
      "01200000 2011.01 41.6588 -73.5287 456       3.33  0.92   0.1074194  0.0378449..\n",
      "01200000 2011.02 41.6588 -73.5287 456       3.45  1.09   0.1232143  0.0569861..\n",
      "..\n"
     ]
    }
   ],
   "source": [
    "constcols:(til count[cols basin]) except raze \n",
    "    {where (count each ss[;x]each string each cols basin)<>0}each (\"2009\";\"2010\";\"2011\")\n",
    "\n",
    "basinupd:flip (cols basin)[constcols]!basin[(cols basin)[constcols]]\n",
    "\n",
    "show joinedtab:rainmonth ij `site_no xkey basinupd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLCD Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join NLCD (impervious info) to each station based on date. Only 3 datasets so link back in time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only pick columns with some variance between stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlcd06:update site_no:`${$[7=count x;\"0\",x;x]}each string each site_no,year:6 from select\n",
    " site_no:SOURCE_FEA,INTPTLAT,INTPTLON,Measure,REACHCODE,distance,imp:imp_nlcd_2006 from nlcd06\n",
    "nlcd11:update site_no:`${$[7=count x;\"0\",x;x]}each string each site_no,year:11 from select\n",
    " site_no:SOURCE_FEA,INTPTLAT,INTPTLON,Measure,REACHCODE,distance,imp:imp_nlcd_2011 from nlcd11\n",
    "nlcd16:update site_no:`${$[7=count x;\"0\",x;x]}each string each site_no,year:16 from select\n",
    " site_no:SOURCE_FEA,INTPTLAT,INTPTLON,Measure,REACHCODE,distance,imp:imp_nlcd_2016 from nlcd16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stryear:{$[x<2011;6;x<2016;11;16]}each `year$joinedtab[`date]\n",
    "\n",
    "merged:update year:stryear from joinedtab\n",
    "\n",
    "newjoinedtab:merged ij `site_no xkey (nlcd06,nlcd11,nlcd16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the Target Data\n",
    "\n",
    "Get the count per month that a station goes over a flood level, broken up into 4 categories-Action,Flood,Moderate and Major"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "/Some gages have no threshold values so had to delete them from dataset\n",
    "dela:first asc exec i by Action from warnings\n",
    "delmj:first 1_asc exec i by Major from warnings\n",
    "delmd:first 2_asc exec i by Moderate from warnings\n",
    "delfl:first 2_asc exec i by Flood from warnings\n",
    "\n",
    "warning:update nn: i from select from warnings where not i in distinct (dela,delmj,delmd,delfl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join based on nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabw:.ml.clust.kd.buildtree[warnlatl:gages[`dec_lat_va`dec_long_v],'\"F\"$'string each warning[`Latitude`Longitude];2]\n",
    "\n",
    "nnwarn:.ml.clust.kd.i.nns[;tabw;(count[warning]#0),count[gages]#1;flip warnlatl;`edist\n",
    "    ]each count[warning]+til count gages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "joins:flip `nn`ndw`site_no!(nnwarn[;0];nnwarn[;1];gages[`site_no])\n",
    "\n",
    "floodlvl:(maxht ij `site_no xkey joins) lj `nn xkey warning\n",
    "\n",
    "floodlvl[`Action`Moderate`Flood`Major]:\"F\"$'floodlvl[`Action`Moderate`Flood`Major]\n",
    "\n",
    "floodlvl[`site_no]:`${$[7=count x;\"0\",x;x]}each string each floodlvl[`site_no] /pad with 0 if len site=7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "site_no  date    Action Flood Moderate Major no_Action no_Flood no_Mod no_Major\n",
      "-------------------------------------------------------------------------------\n",
      "01200000 2009.07 17     19    22       24    0         0        0      0       \n",
      "01200000 2009.08 17     19    22       24    0         0        0      0       \n",
      "01200000 2009.09 17     19    22       24    0         0        0      0       \n",
      "01200000 2009.10 17     19    22       24    0         0        0      0       \n",
      "01200000 2009.11 17     19    22       24    0         0        0      0       \n",
      "01200000 2009.12 17     19    22       24    0         0        0      0       \n",
      "01200000 2010.01 17     19    22       24    0         0        0      0       \n",
      "01200000 2010.02 17     19    22       24    0         0        0      0       \n",
      "01200000 2010.03 17     19    22       24    0         0        0      0       \n",
      "01200000 2010.04 17     19    22       24    0         0        0      0       \n",
      "01200000 2010.05 17     19    22       24    0         0        0      0       \n",
      "01200000 2010.06 17     19    22       24    0         0        0      0       \n",
      "01200000 2010.07 17     19    22       24    0         0        0      0       \n",
      "01200000 2010.08 17     19    22       24    0         0        0      0       \n",
      "01200000 2010.09 17     19    22       24    0         0        0      0       \n",
      "01200000 2010.10 17     19    22       24    0         0        0      0       \n",
      "01200000 2010.11 17     19    22       24    0         0        0      0       \n",
      "01200000 2010.12 17     19    22       24    0         0        0      0       \n",
      "01200000 2011.01 17     19    22       24    0         0        0      0       \n",
      "01200000 2011.02 17     19    22       24    0         0        0      0       \n",
      "..\n"
     ]
    }
   ],
   "source": [
    "show target:0!select distinct Action,distinct Flood,distinct Moderate,distinct Major,no_Action:count where height>Action\n",
    " ,no_Flood:count where height>Flood,no_Mod:count where height>Moderate,no_Major:count where height>Major by site_no\n",
    "    ,\"m\"$date from floodlvl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "site_no  date    lat     long     elevation sumpr maxpre avgpre     varpre   ..\n",
      "-----------------------------------------------------------------------------..\n",
      "01200000 2009.07 41.6588 -73.5287 456       8.01  2.04   0.2583871  0.2031039..\n",
      "01200000 2009.08 41.6588 -73.5287 456       5.94  1.59   0.1916129  0.1258458..\n",
      "01200000 2009.09 41.6588 -73.5287 456       1.55  0.85   0.05166667 0.0267205..\n",
      "01200000 2009.10 41.6588 -73.5287 456       4.51  1.46   0.1454839  0.1004828..\n",
      "01200000 2009.11 41.6588 -73.5287 456       1.52  0.49   0.05066667 0.0135128..\n",
      "01200000 2009.12 41.6588 -73.5287 456       5.61  0.93   0.1809677  0.0881893..\n",
      "01200000 2010.01 41.6588 -73.5287 456       2.82  1.75   0.09096774 0.0974539..\n",
      "01200000 2010.02 41.6588 -73.5287 456       4.41  2.28   0.1575     0.1941045..\n",
      "01200000 2010.03 41.6588 -73.5287 456       6.5   1.36   0.2096774  0.1715322..\n",
      "01200000 2010.04 41.6588 -73.5287 456       2.17  0.57   0.07233333 0.0186045..\n",
      "01200000 2010.05 41.6588 -73.5287 456       1.93  0.69   0.06225806 0.0185465..\n",
      "01200000 2010.06 41.6588 -73.5287 456       3.12  0.73   0.104      0.0336373..\n",
      "01200000 2010.07 41.6588 -73.5287 456       2.39  0.56   0.07709677 0.0247496..\n",
      "01200000 2010.08 41.6588 -73.5287 456       5.81  2.7    0.1874194  0.2565869..\n",
      "01200000 2010.09 41.6588 -73.5287 456       1.95  0.74   0.065      0.0303916..\n",
      "01200000 2010.10 41.6588 -73.5287 456       8.96  3.1    0.2890323  0.5052797..\n",
      "01200000 2010.11 41.6588 -73.5287 456       3.19  1.15   0.1063333  0.0734098..\n",
      "01200000 2010.12 41.6588 -73.5287 456       4.74  1.8    0.1529032  0.19124  ..\n",
      "01200000 2011.01 41.6588 -73.5287 456       3.33  0.92   0.1074194  0.0378449..\n",
      "01200000 2011.02 41.6588 -73.5287 456       3.45  1.09   0.1232143  0.0569861..\n",
      "..\n"
     ]
    }
   ],
   "source": [
    "show allmerged:newjoinedtab ij `site_no`date xkey target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "/Tidy up some column types\n",
    "allmerged[`lat`long]:raze each \"F\"$'string each allmerged[`lat`long]\n",
    "allmerged[`Action]:raze allmerged[`Action]\n",
    "allmerged[`Flood]:raze allmerged[`Flood]\n",
    "allmerged[`Moderate]:raze allmerged[`Moderate]\n",
    "allmerged[`Major]:raze allmerged[`Major]\n",
    "allmerged[`elevation]:raze allmerged[`elevation]\n",
    "\n",
    "/add cosin of month\n",
    "allmerged:update month:`mm$date,cos_t:cos 2*3.14*(`mm$date)%12,sin_t:sin 2*3.14*(`mm$date)%12 from allmerged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Features Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add lagged features. Previous data about rainfall/stream gages are also an important feature needed to measure if a river will flood or not. The lagged stream features that will be extracted are:\n",
    "\n",
    "      - The stream gage level was at a given stream a year prior\n",
    "      - The average gage level over the last 3 months \n",
    "      - The average stream level over it's lifetime until the given date\n",
    "      \n",
    "\n",
    "The lagged rainfall features that will be extracted are:\n",
    "    \n",
    "       -The max,avg and variance rainfall for a given streamgage for the previous 1,2 and 3 months\n",
    "       -The rainfall that occured at the 3 gages upstream at that given time (The USGS orders the stream numbers in an ascending order. The first 2 digits correspond with the basin the stream belongs to, while the remaining digits are the order that the given basin appears in the stream (upstream to downstream)\n",
    "       \n",
    " \n",
    "NOTE: This model is strictly for use on streams with gage history. These features will not be applied to ungaged basins. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date    site_no  lat     long     elevation sumpr maxpre avgpre    varpre    ..\n",
      "-----------------------------------------------------------------------------..\n",
      "2009.07 01200000 41.6588 -73.5287 456       8.01  2.04   0.2583871 0.2031039 ..\n",
      "2009.07 01302020 40.8623 -73.8744 115       5.64  0.9    0.1819355 0.06038335..\n",
      "2009.07 01303000 40.8875 -73.5636 118       4.72  1.24   0.1522581 0.06770135..\n",
      "2009.07 01303500 40.8572 -73.4633 112       4.49  1.22   0.1448387 0.08244433..\n",
      "2009.07 01304000 40.8494 -73.2242 59        5.59  2.14   0.1803226 0.2114805 ..\n",
      "2009.07 01304500 40.9136 -72.6867 20        5.77  2.09   0.186129  0.1817528 ..\n",
      "2009.07 01305000 40.8303 -72.9061 62        6.31  2.65   0.2035484 0.2794874 ..\n",
      "2009.07 01305500                                                             ..\n",
      "2009.07 01306460 40.7719 -73.1586 52        6.21  2.19   0.2003226 0.2373128 ..\n",
      "2009.07 01308000 40.7042 -73.3139 20        4.97  1.38   0.1603226 0.1314289 ..\n",
      "2009.07 01308500 40.7086 -73.3283 20        4.97  1.38   0.1603226 0.1314289 ..\n",
      "2009.07 01309500 40.6889 -73.4547 49        4.88  1.34   0.1574194 0.1072901 ..\n",
      "2009.07 01310500                                                             ..\n",
      "2009.07 01311500 40.6636 -73.7044 20        4.43  0.64   0.1429032 0.04002706..\n",
      "2009.07 01312000 43.9667 -74.1317 1637      4.8   0.79   0.1548387 0.04181852..\n",
      "2009.07 01315000 43.7564 -74.2672 1680      5.05  1.26   0.1629032 0.09910447..\n",
      "2009.07 01315500 43.7008 -73.9833 1293      5.41  1.31   0.1745161 0.09729573..\n",
      "2009.07 01317000 43.6094 -73.7375 951       6.83  1.46   0.2203226 0.1427967 ..\n",
      "2009.07 01318500 43.3189 -73.8442 735       7.3   2.25   0.2354839 0.2357344 ..\n",
      "2009.07 01321000 43.3528 -74.2703 1158      4.87  1.47   0.1570968 0.1080335 ..\n",
      "..\n"
     ]
    }
   ],
   "source": [
    "/Join all dates within time range with stream sites so that there is no gaps in the time series\n",
    "all_dt_range:([] date:raze (count[distinct allmerged[`site_no]])#'asc distinct allmerged[`date];\n",
    "    site_no:raze flip (count[distinct allmerged[`date]])#'asc distinct allmerged[`site_no])\n",
    "\n",
    "show all_dt_merge:all_dt_range lj `site_no`date xkey allmerged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain the previous stream levels of each site in history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date    site_no  lagged_target_1yr lagged_target_recent lagged_target_all\n",
      "-------------------------------------------------------------------------\n",
      "2010.07 01200000 0                 0                    0                \n",
      "2010.08 01200000 0                 0                    0                \n",
      "2010.09 01200000 0                 0                    0                \n",
      "2010.10 01200000 0                 0                    0                \n",
      "2010.11 01200000 0                 0                    0                \n",
      "2010.12 01200000 0                 0                    0                \n",
      "2011.01 01200000 0                 0                    0                \n",
      "2011.02 01200000 0                 0                    0                \n",
      "2011.03 01200000 0                 0                    0                \n",
      "2011.04 01200000 0                 0                    0                \n",
      "2011.05 01200000 0                 0                    0                \n",
      "2011.06 01200000 0                 0                    0                \n",
      "2011.07 01200000 0                 0                    0                \n",
      "2011.08 01200000 0                 0                    0                \n",
      "2011.09 01200000 0                 0                    0                \n",
      "2011.10 01200000 0                 0                    0                \n",
      "2011.11 01200000 0                 0                    0                \n",
      "2011.12 01200000 0                 0                    0                \n",
      "2012.01 01200000 0                 0                    0                \n",
      "2012.02 01200000 0                 0                    0                \n",
      "..\n"
     ]
    }
   ],
   "source": [
    "catch_tgts:0!select site_no,no_Flood,date,cs:count date by site_no from all_dt_merge\n",
    "\n",
    "lagy:raze{12 xprev raze x}each exec no_Flood from catch_tgts\n",
    "lag1:raze{1 xprev raze x}each exec no_Flood from catch_tgts\n",
    "lag2:raze{2 xprev raze x}each exec no_Flood from catch_tgts\n",
    "lag3:raze{3 xprev raze x}each exec no_Flood from catch_tgts\n",
    "lagall:raze{count[x] mavg raze x}each exec no_Flood from catch_tgts\n",
    "\n",
    "lagavg:avg each lag1,'lag2,'lag3\n",
    "\n",
    "lagdt:([] date:raze ((catch_tgts[`cs])#'catch_tgts[`date]);site_no:raze (catch_tgts[`cs])#'catch_tgts[`site_no];\n",
    "    lagged_target_1yr:lagy;lagged_target_recent:lag3;lagged_target_all:lagall)\n",
    "\n",
    "show lagdt:delete from lagdt where i in where any flip null value each lagdt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain the 3 month rainfall history per site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date    site_no  avg_prev_1 avg_prev_2 avg_prev_3 max_prev_1 max_prev_2 max_p..\n",
      "-----------------------------------------------------------------------------..\n",
      "2009.10 01200000 0.05166667 0.1916129  0.2583871  0.85       1.59       2.04 ..\n",
      "2009.11 01200000 0.1454839  0.05166667 0.1916129  1.46       0.85       1.59 ..\n",
      "2009.12 01200000 0.05066667 0.1454839  0.05166667 0.49       1.46       0.85 ..\n",
      "2010.01 01200000 0.1809677  0.05066667 0.1454839  0.93       0.49       1.46 ..\n",
      "2010.02 01200000 0.09096774 0.1809677  0.05066667 1.75       0.93       0.49 ..\n",
      "2010.03 01200000 0.1575     0.09096774 0.1809677  2.28       1.75       0.93 ..\n",
      "2010.04 01200000 0.2096774  0.1575     0.09096774 1.36       2.28       1.75 ..\n",
      "2010.05 01200000 0.07233333 0.2096774  0.1575     0.57       1.36       2.28 ..\n",
      "2010.06 01200000 0.06225806 0.07233333 0.2096774  0.69       0.57       1.36 ..\n",
      "2010.07 01200000 0.104      0.06225806 0.07233333 0.73       0.69       0.57 ..\n",
      "2010.08 01200000 0.07709677 0.104      0.06225806 0.56       0.73       0.69 ..\n",
      "2010.09 01200000 0.1874194  0.07709677 0.104      2.7        0.56       0.73 ..\n",
      "2010.10 01200000 0.065      0.1874194  0.07709677 0.74       2.7        0.56 ..\n",
      "2010.11 01200000 0.2890323  0.065      0.1874194  3.1        0.74       2.7  ..\n",
      "2010.12 01200000 0.1063333  0.2890323  0.065      1.15       3.1        0.74 ..\n",
      "2011.01 01200000 0.1529032  0.1063333  0.2890323  1.8        1.15       3.1  ..\n",
      "2011.02 01200000 0.1074194  0.1529032  0.1063333  0.92       1.8        1.15 ..\n",
      "2011.03 01200000 0.1232143  0.1074194  0.1529032  1.09       0.92       1.8  ..\n",
      "2011.04 01200000 0.2216129  0.1232143  0.1074194  3.2        1.09       0.92 ..\n",
      "2011.05 01200000 0.1863333  0.2216129  0.1232143  1.98       3.2        1.09 ..\n",
      "..\n"
     ]
    }
   ],
   "source": [
    "catch_dts:0!select date,avgpre,maxpre,varpre,cs:count i by site_no from all_dt_merge\n",
    "\n",
    "avg1:raze {1 xprev raze x}each exec avgpre from catch_dts\n",
    "avg2:raze {2 xprev raze x}each exec avgpre from catch_dts\n",
    "avg3:raze {3 xprev raze x}each exec avgpre from catch_dts\n",
    "\n",
    "\n",
    "max1:raze {1 xprev raze x}each exec maxpre from catch_dts\n",
    "max2:raze{2 xprev raze x}each exec maxpre from catch_dts\n",
    "max3:raze{3 xprev raze x}each exec maxpre from catch_dts\n",
    "\n",
    "var1:raze{1 xprev raze x}each exec varpre from catch_dts\n",
    "var2:raze{2 xprev raze x}each exec varpre from catch_dts\n",
    "var3:raze{3 xprev raze x}each exec varpre from catch_dts\n",
    "\n",
    "prevdt:([] date:raze ((catch_dts[`cs])#'catch_dts[`date]);site_no:raze (catch_dts[`cs])#'catch_dts[`site_no];\n",
    "    avg_prev_1:avg1;\n",
    "    avg_prev_2:avg2;avg_prev_3:avg3;\n",
    "    max_prev_1:max1;max_prev_2:max2;max_prev_3:max3;var_prev_1:var1;var_prev_2:var2;var_prev_3:var3)\n",
    "\n",
    "show prevdt:delete from prevdt where i in where any flip null value each prevdt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain the rainfall up the 3 upstream basins of a site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date    site_no  avg_ups_1 avg_ups_2 avg_ups_3 max_ups_1 max_ups_2 max_ups_3 ..\n",
      "-----------------------------------------------------------------------------..\n",
      "2009.07 01303500 0.1522581 0.1819355 0.2583871 1.24      0.9       2.04      ..\n",
      "2009.07 01304000 0.1448387 0.1522581 0.1819355 1.22      1.24      0.9       ..\n",
      "2009.07 01304500 0.1803226 0.1448387 0.1522581 2.14      1.22      1.24      ..\n",
      "2009.07 01305000 0.186129  0.1803226 0.1448387 2.09      2.14      1.22      ..\n",
      "2009.07 01305500 0.2035484 0.186129  0.1803226 2.65      2.09      2.14      ..\n",
      "2009.07 01309500 0.1603226 0.1603226 0.2003226 1.38      1.38      2.19      ..\n",
      "2009.07 01310500 0.1574194 0.1603226 0.1603226 1.34      1.38      1.38      ..\n",
      "2009.07 01315500 0.1629032 0.1548387 0.1429032 1.26      0.79      0.64      ..\n",
      "2009.07 01317000 0.1745161 0.1629032 0.1548387 1.31      1.26      0.79      ..\n",
      "2009.07 01318500 0.2203226 0.1745161 0.1629032 1.46      1.31      1.26      ..\n",
      "2009.07 01321000 0.2354839 0.2203226 0.1745161 2.25      1.46      1.31      ..\n",
      "2009.07 01325000 0.1570968 0.2354839 0.2203226 1.47      2.25      1.46      ..\n",
      "2009.07 01327750 0.2174194 0.1570968 0.2354839 2.11      1.47      2.25      ..\n",
      "2009.07 01329490 0.2032258 0.2174194 0.1570968 1.54      2.11      1.47      ..\n",
      "2009.07 01330000 0.2474194 0.2032258 0.2174194 1.82      1.54      2.11      ..\n",
      "2009.07 01334500 0.1864516 0.2474194 0.2032258 1.32      1.82      1.54      ..\n",
      "2009.07 01335754 0.2290323 0.1864516 0.2474194 1.55      1.32      1.82      ..\n",
      "2009.07 01335755 0.2648387 0.2290323 0.1864516 3.02      1.55      1.32      ..\n",
      "2009.07 01348000 0.1645161 0.1303226 0.1519355 1.03      0.71      0.98      ..\n",
      "2009.07 01349000 0.1735484 0.1645161 0.1303226 1.18      1.03      0.71      ..\n",
      "..\n"
     ]
    }
   ],
   "source": [
    "catch_atts:0!select site_no,avgpre,maxpre,varpre,cs:count i by date,catch:2#'string each site_no from all_dt_merge\n",
    "\n",
    "avg1:raze {1 xprev raze x}each exec avgpre from catch_atts\n",
    "avg2:raze {2 xprev raze x}each exec avgpre from catch_atts\n",
    "avg3:raze {3 xprev raze x}each exec avgpre from catch_atts\n",
    "\n",
    "\n",
    "max1:raze {1 xprev raze x}each exec maxpre from catch_atts\n",
    "max2:raze{2 xprev raze x}each exec maxpre from catch_atts\n",
    "max3:raze{3 xprev raze x}each exec maxpre from catch_atts\n",
    "\n",
    "var1:raze{1 xprev raze x}each exec varpre from catch_atts\n",
    "var2:raze{2 xprev raze x}each exec varpre from catch_atts\n",
    "var3:raze{3 xprev raze x}each exec varpre from catch_atts\n",
    "\n",
    "upstr:([] date:raze ((catch_atts[`cs])#'catch_atts[`date]);site_no:raze catch_atts[`site_no]; avg_ups_1:avg1;\n",
    "    avg_ups_2:avg2;avg_ups_3:avg3;\n",
    "    max_ups_1:max1;max_ups_2:max2;max_ups_3:max3;var_ups_1:var1;var_ups_2:var2;var_ups_3:var3)\n",
    "\n",
    "show upstr:delete from upstr where i in where any flip null value each upstr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join all the columns together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "site_no  date    lat     long     elevation sumpr maxpre avgpre     varpre   ..\n",
      "-----------------------------------------------------------------------------..\n",
      "01303500 2010.07 40.8572 -73.4633 112       2.42  0.57   0.07806452 0.0261510..\n",
      "01303500 2010.08 40.8572 -73.4633 112       2.1   1.58   0.06774194 0.0788110..\n",
      "01303500 2010.09 40.8572 -73.4633 112       3.11  1.15   0.1036667  0.0659032..\n",
      "01303500 2013.12 40.8572 -73.4633 112       4.7   1.33   0.1516129  0.1197039..\n",
      "01303500 2014.01 40.8572 -73.4633 112       3     0.54   0.09677419 0.0265831..\n",
      "01303500 2014.02 40.8572 -73.4633 112       4.32  1.13   0.1542857  0.0716459..\n",
      "01303500 2014.03 40.8572 -73.4633 112       4.18  2.69   0.1348387  0.2420895..\n",
      "01303500 2014.04 40.8572 -73.4633 112       3.21  0.96   0.107      0.0531343..\n",
      "01303500 2014.05 40.8572 -73.4633 112       6.79  3.69   0.2190323  0.4428152..\n",
      "01303500 2014.06 40.8572 -73.4633 112       3.16  0.9    0.1053333  0.0478448..\n",
      "01303500 2014.07 40.8572 -73.4633 112       3.04  1.1    0.09806452 0.0502349..\n",
      "01303500 2014.08 40.8572 -73.4633 112       3.73  2.72   0.1203226  0.2344225..\n",
      "01303500 2014.09 40.8572 -73.4633 112       2.38  0.82   0.07933333 0.0373795..\n",
      "01303500 2014.10 40.8572 -73.4633 112       3.89  1.07   0.1254839  0.0600570..\n",
      "01303500 2014.11 40.8572 -73.4633 112       4.56  1      0.152      0.081856 ..\n",
      "01303500 2014.12 40.8572 -73.4633 112       6     1.88   0.1935484  0.1661003..\n",
      "01303500 2015.01 40.8572 -73.4633 112       5.2   1.54   0.1677419  0.1190433..\n",
      "01303500 2015.02 40.8572 -73.4633 112       2.15  0.75   0.07678571 0.0360003..\n",
      "01303500 2015.03 40.8572 -73.4633 112       4.43  0.68   0.1429032  0.0482399..\n",
      "01303500 2015.04 40.8572 -73.4633 112       1.95  1.04   0.065      0.0360783..\n",
      "..\n"
     ]
    }
   ],
   "source": [
    "show added_cols:((allmerged ij `site_no`date xkey upstr) ij `site_no`date xkey prevdt) ij `site_no`date xkey lagdt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgts:select site_no,no_Action,no_Flood,no_Mod,no_Major from added_cols\n",
    "added_cols:delete no_Action,no_Flood,no_Mod,no_Major from added_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "site_no  date   | lat     long     elevation sumpr maxpre avgpre     varpre  ..\n",
      "----------------| -----------------------------------------------------------..\n",
      "01303500 2010.07| 40.8572 -73.4633 112       2.42  0.57   0.07806452 0.026151..\n",
      "01303500 2010.08| 40.8572 -73.4633 112       2.1   1.58   0.06774194 0.078811..\n",
      "01303500 2010.09| 40.8572 -73.4633 112       3.11  1.15   0.1036667  0.065903..\n",
      "01303500 2013.12| 40.8572 -73.4633 112       4.7   1.33   0.1516129  0.119703..\n",
      "01303500 2014.01| 40.8572 -73.4633 112       3     0.54   0.09677419 0.026583..\n",
      "01303500 2014.02| 40.8572 -73.4633 112       4.32  1.13   0.1542857  0.071645..\n",
      "01303500 2014.03| 40.8572 -73.4633 112       4.18  2.69   0.1348387  0.242089..\n",
      "01303500 2014.04| 40.8572 -73.4633 112       3.21  0.96   0.107      0.053134..\n",
      "01303500 2014.05| 40.8572 -73.4633 112       6.79  3.69   0.2190323  0.442815..\n",
      "01303500 2014.06| 40.8572 -73.4633 112       3.16  0.9    0.1053333  0.047844..\n",
      "01303500 2014.07| 40.8572 -73.4633 112       3.04  1.1    0.09806452 0.050234..\n",
      "01303500 2014.08| 40.8572 -73.4633 112       3.73  2.72   0.1203226  0.234422..\n",
      "01303500 2014.09| 40.8572 -73.4633 112       2.38  0.82   0.07933333 0.037379..\n",
      "01303500 2014.10| 40.8572 -73.4633 112       3.89  1.07   0.1254839  0.060057..\n",
      "01303500 2014.11| 40.8572 -73.4633 112       4.56  1      0.152      0.081856..\n",
      "01303500 2014.12| 40.8572 -73.4633 112       6     1.88   0.1935484  0.166100..\n",
      "01303500 2015.01| 40.8572 -73.4633 112       5.2   1.54   0.1677419  0.119043..\n",
      "01303500 2015.02| 40.8572 -73.4633 112       2.15  0.75   0.07678571 0.036000..\n",
      "01303500 2015.03| 40.8572 -73.4633 112       4.43  0.68   0.1429032  0.048239..\n",
      "01303500 2015.04| 40.8572 -73.4633 112       1.95  1.04   0.065      0.036078..\n",
      "..\n"
     ]
    }
   ],
   "source": [
    "show tabreduced:`site_no`date xkey .ml.filltab[added_cols;`site_no;`date;::]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Auto-ML we were able to extract the most important basin characteristics from the model, the lagged features were then added after "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "/cols to include\n",
    "cols_to_include:\n",
    "    `site_no`date`cos_t`sin_t,\n",
    "    `elevation`imp`avgpre`maxpre`CatAreaSqKm`WsAreaSqKm`CatAreaSqKmRp100`WsAreaSqKmRp100,\n",
    "    `ElevCat`ElevWs`WtDepCat`WtDepWs`OmCat`OmWs`PermCat`PermWs`RckDepCat`RckDepWs`ClayCat`ClayWs,\n",
    "    `SandCat`SandWs`RunoffCat`RunoffWs`WetIndexCat`WetIndexWs`BFICat`BFIWs,\n",
    "    `avg_prev_1`avg_prev_2`avg_prev_3`max_prev_1`max_prev_2`max_prev_3`var_prev_1`var_prev_2,\n",
    "    `var_prev_3`avg_ups_1`avg_ups_2`avg_ups_3`max_ups_1`max_ups_2`max_ups_3`var_ups_1,`var_ups_2`var_ups_3,\n",
    "    `lagged_target_1yr`lagged_target_recent`lagged_target_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "site_no  date   | cos_t        sin_t        elevation imp      avgpre     max..\n",
      "----------------| -----------------------------------------------------------..\n",
      "01303500 2010.07| -0.866953    -0.49839     112       14.47757 0.07806452 0.5..\n",
      "01303500 2010.08| -0.5018379   -0.8649617   112       14.47757 0.06774194 1.5..\n",
      "01303500 2010.09| -0.002388978 -0.9999971   112       14.47757 0.1036667  1.1..\n",
      "01303500 2013.12| 0.9999949    -0.003185302 112       14.47757 0.1516129  1.3..\n",
      "01303500 2014.01| 0.8661581    0.4997701    112       14.47757 0.09677419 0.5..\n",
      "01303500 2014.02| 0.5004597    0.8657598    112       14.47757 0.1542857  1.1..\n",
      "01303500 2014.03| 0.0007963267 0.9999997    112       14.47757 0.1348387  2.6..\n",
      "01303500 2014.04| -0.4990802   0.8665558    112       14.47757 0.107      0.9..\n",
      "01303500 2014.05| -0.865361    0.501149     112       14.47757 0.2190323  3.6..\n",
      "01303500 2014.06| -0.9999987   0.001592653  112       14.47757 0.1053333  0.9..\n",
      "01303500 2014.07| -0.866953    -0.49839     112       14.47757 0.09806452 1.1..\n",
      "01303500 2014.08| -0.5018379   -0.8649617   112       14.47757 0.1203226  2.7..\n",
      "01303500 2014.09| -0.002388978 -0.9999971   112       14.47757 0.07933333 0.8..\n",
      "01303500 2014.10| 0.4976994    -0.8673496   112       14.47757 0.1254839  1.0..\n",
      "01303500 2014.11| 0.8645618    -0.5025265   112       14.47757 0.152      1  ..\n",
      "01303500 2014.12| 0.9999949    -0.003185302 112       14.47757 0.1935484  1.8..\n",
      "01303500 2015.01| 0.8661581    0.4997701    112       14.47757 0.1677419  1.5..\n",
      "01303500 2015.02| 0.5004597    0.8657598    112       14.47757 0.07678571 0.7..\n",
      "01303500 2015.03| 0.0007963267 0.9999997    112       14.47757 0.1429032  0.6..\n",
      "01303500 2015.04| -0.4990802   0.8665558    112       14.47757 0.065      1.0..\n",
      "..\n"
     ]
    }
   ],
   "source": [
    "show tabreducedn:`site_no`date xkey flip cols_to_include!(0!tabreduced)[cols_to_include]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardised scaling is preformed on some of the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdtab:.ml.stdscaler[value tabreducedn]\n",
    "newtab:`site_no`date xkey (select site_no,date from 0!tabreducedn),'stdtab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train and test set are strictly divided so that each station is either in train or test so that there is no data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tstsplt:250#distinct key[newtab][`site_no]\n",
    "trainsplt:250_distinct key[newtab][`site_no]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain:value select from newtab where site_no in trainsplt\n",
    "xtest:value select from newtab where site_no in tstsplt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "/predict for  Moderate floods\n",
    "ytrain:(exec no_Flood from tgts where site_no in trainsplt)>0\n",
    "ytest:(exec no_Flood from tgts where site_no in tstsplt)>0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "/backfilling per site doesn't work as nulls for all site, will look more into that\n",
    "xtr:(mattab xtrain)[(til count[mattab xtrain]) except where 0<>{count where x=0n}each mattab xtrain]\n",
    "ytr:ytrain[(til count[mattab xtrain]) except where 0<>{count where x=0n}each mattab xtrain]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf:.p.import[`sklearn.ensemble][`:RandomForestClassifier][`n_estimators pykw 200;`random_state pykw 1]\n",
    "clf[`:fit][xtr;ytr]`;\n",
    "pred1:clf[`:predict][mattab xtest]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class    | precision recall    f1_score  support\n",
       "---------| -------------------------------------\n",
       "0        | 0.997344  0.9778935 0.987523  17280  \n",
       "1        | 0.8003136 0.9714467 0.8776154 1576   \n",
       "avg/total| 0.8988288 0.9746701 0.9325692 18856  \n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ".ml.classreport[ytest;pred1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0| 16898 382 \n",
      "1| 45    1531\n",
      "The accuracy of the model is 0.9773547\n",
      "The mean_class_accuracy of the model is 0.9746701\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show cnfM:.ml.confmat[ytest;pred1]\n",
    "conf:.ml.confdict[ytest;pred1;1b]\n",
    "\n",
    "acc:(count where pred1=ytest)%count[ytest]\n",
    "meanclassavg:avg (conf[`tp]%(sum conf[`tp`fn]);conf[`tn]%(sum conf[`tn`fp]))\n",
    "\n",
    "-1\"The accuracy of the model is \",string acc;\n",
    "-1\"The mean_class_accuracy of the model is \",string meanclassavg;\n",
    ".ml.displayCM[value cnfM;`Low`High;\"Test Set Confusion Matrix\";()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import models needed for Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "tf: .p.import[`tensorflow]\n",
    "ker: .p.import[`tensorflow.keras]\n",
    "layers:.p.import[`tensorflow.keras.layers]\n",
    "dense:  .p.import[`keras.layers]`:Dense\n",
    "\n",
    "km:.p.import[`keras_metrics]\n",
    "models:.p.import[`keras]`:Model\n",
    "sequential:.p.import[`keras.models]`:Sequential\n",
    "plt:.p.import[`matplotlib]`:pyplot\n",
    "inp:.p.import[`keras.layers]`:Input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0801 05:19:31.164083 140643687219200 deprecation_wrapper.py:119] From /home/dianeodonoghue/miniconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0801 05:19:31.190910 140643687219200 deprecation_wrapper.py:119] From /home/dianeodonoghue/miniconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0801 05:19:31.198262 140643687219200 deprecation_wrapper.py:119] From /home/dianeodonoghue/miniconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0801 05:19:31.273861 140643687219200 deprecation_wrapper.py:119] From /home/dianeodonoghue/miniconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0801 05:19:31.302393 140643687219200 deprecation_wrapper.py:119] From /home/dianeodonoghue/miniconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0801 05:19:31.308670 140643687219200 deprecation.py:323] From /home/dianeodonoghue/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0801 05:19:31.343495 140643687219200 deprecation_wrapper.py:119] From /home/dianeodonoghue/miniconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 51)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 40)                2080      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 40)                1640      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 41        \n",
      "=================================================================\n",
      "Total params: 3,761\n",
      "Trainable params: 3,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-08-01 05:19:31.813399: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2019-08-01 05:19:31.827947: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
      "2019-08-01 05:19:31.832132: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x6d146d0 executing computations on platform Host. Devices:\n",
      "2019-08-01 05:19:31.832241: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "2019-08-01 05:19:32.049280: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "23660/23660 [==============================] - 1s 40us/step - loss: 0.2206 - precision: 0.9510 - recall: 0.62350e+ - ETA: 4s - loss: 0.4784 - precision: 0.9600 - recall: 0.0789         - ETA: 2s - loss: 0.4100 - precision: 0.9753 - recall: 0.24 - ETA: 1s - loss: 0.3754 - precision: 0.9814 - recall: 0.33 - ETA: 1s - loss: 0.3385 - precision: 0.9765 - recall: 0.40 - ETA: 0s - loss: 0.3117 - precision: 0.9724 - recall: 0.45 - ETA: 0s - loss: 0.2909 - precision: 0.9670 - recall: 0.49 - ETA: 0s - loss: 0.2721 - precision: 0.9595 - recall: 0.52 - ETA: 0s - loss: 0.2571 - precision: 0.9582 - recall: 0.55 - ETA: 0s - loss: 0.2482 - precision: 0.9575 - recall: 0.57 - ETA: 0s - loss: 0.2406 - precision: 0.9573 - recall: 0.58 - ETA: 0s - loss: 0.2357 - precision: 0.9532 - recall: 0.59 - ETA: 0s - loss: 0.2285 - precision: 0.9543 - recall: 0.61 - ETA: 0s - loss: 0.2239 - precision: 0.9531 - recall: 0.61\n",
      "Epoch 2/100\n",
      "23660/23660 [==============================] - ETA: 1s - loss: 0.1389 - precision: 0.8571 - recall: 0.85 - ETA: 0s - loss: 0.1524 - precision: 0.9237 - recall: 0.74 - ETA: 0s - loss: 0.1548 - precision: 0.9310 - recall: 0.74 - ETA: 0s - loss: 0.1545 - precision: 0.9289 - recall: 0.74 - ETA: 0s - loss: 0.1531 - precision: 0.9295 - recall: 0.75 - ETA: 0s - loss: 0.1576 - precision: 0.9319 - recall: 0.74 - ETA: 0s - loss: 0.1563 - precision: 0.9333 - recall: 0.75 - ETA: 0s - loss: 0.1552 - precision: 0.9317 - recall: 0.75 - ETA: 0s - loss: 0.1548 - precision: 0.9287 - recall: 0.75 - ETA: 0s - loss: 0.1541 - precision: 0.9294 - recall: 0.75 - ETA: 0s - loss: 0.1541 - precision: 0.9270 - recall: 0.75 - ETA: 0s - loss: 0.1515 - precision: 0.9295 - recall: 0.75 - ETA: 0s - loss: 0.1493 - precision: 0.9297 - recall: 0.76 - 1s 27us/step - loss: 0.1479 - precision: 0.9316 - recall: 0.7653\n",
      "Epoch 3/100\n",
      "23660/23660 [==============================] - 1s 26us/step - loss: 0.1332 - precision: 0.9273 - recall: 0.7922 0s - loss: 0.1416 - precision: 0.9135 - recall: 0.75 - ETA: 0s - loss: 0.1379 - precision: 0.9073 - recall: 0.76 - ETA: 0s - loss: 0.1341 - precision: 0.9209 - recall: 0.76 - ETA: 0s - loss: 0.1354 - precision: 0.9209 - recall: 0.76 - ETA: 0s - loss: 0.1367 - precision: 0.9200 - recall: 0.77 - ETA: 0s - loss: 0.1363 - precision: 0.9243 - recall: 0.78 - ETA: 0s - loss: 0.1363 - precision: 0.9241 - recall: 0.78 - ETA: 0s - loss: 0.1357 - precision: 0.9248 - recall: 0.78 - ETA: 0s - loss: 0.1357 - precision: 0.9245 - recall: 0.78 - ETA: 0s - loss: 0.1352 - precision: 0.9240 - recall: 0.78 - ETA: 0s - loss: 0.1341 - precision: 0.9262 - recall: 0.79\n",
      "Epoch 4/100\n",
      "23660/23660 [==============================] - 1s 22us/step - loss: 0.1235 - precision: 0.9239 - recall: 0.8113 0s - loss: 0.1254 - precision: 0.9238 - recall: 0.81 - ETA: 0s - loss: 0.1281 - precision: 0.9227 - recall: 0.80 - ETA: 0s - loss: 0.1237 - precision: 0.9343 - recall: 0. - ETA: 0s - loss: 0.1244 - precision: 0.9282 - recall: 0.80 - ETA: 0s - loss: 0.1253 - precision: 0.9269 - recall: 0.80 - ETA: 0s - loss: 0.1246 - precision: 0.9236 - recall: 0. - ETA: 0s - loss: 0.1240 - precision: 0.9261 - recall: 0.80\n",
      "Epoch 5/100\n",
      "23660/23660 [==============================] - ETA: 0s - loss: 0.1118 - precision: 0.9323 - recall: 0.8253 ETA: 0s - loss: 0.1057 - precision: 0.9707 - recall: 0.82 - ETA: 0s - loss: 0.1033 - precision: 0.9504 - recall: 0.82 - ETA: 0s - loss: 0.1097 - precision: 0.9404 - recall: 0.82 - ETA: 0s - loss: 0.1093 - precision: 0.9347 - recall: 0. - ETA: 0s - loss: 0.1143 - precision: 0.9299 - recall: 0.82 - ETA: 0s - loss: 0.1161 - precision: 0.9256 - recall: 0.82 - ETA: 0s - loss: 0.1143 - precision: 0.9287 - recall: 0.82 - ETA: 0s - loss: 0.1151 - precision: 0.9311 - recall: 0.82 - ETA: 0s - loss: 0.1163 - precision: 0.9294 - recall: 0.82 - ETA: 0s - loss: 0.1160 - precision: 0.9295 - recall: 0.82 - ETA: 0s - loss: 0.1165 - precision: 0.9297 - recall: 0.82 - 1s 27us/step - loss: 0.1169 - precision: 0.9290 - recall: 0.8263\n",
      "Epoch 6/100\n",
      "23660/23660 [==============================] - ETA: 0s - loss: 0.1204 - precision: 0.8000 - recall: 0.80 - ETA: 0s - loss: 0.1244 - precision: 0.9086 - recall: 0.81 - ETA: 0s - loss: 0.1169 - precision: 0.9224 - recall: 0.82 - ETA: 0s - loss: 0.1159 - precision: 0.9198 - recall: 0.83 - ETA: 0s - loss: 0.1129 - precision: 0.9272 - recall: 0.82 - ETA: 0s - loss: 0.1127 - precision: 0.9325 - recall: 0.82 - ETA: 0s - loss: 0.1108 - precision: 0.9336 - recall: 0.83 - ETA: 0s - loss: 0.1103 - precision: 0.9327 - recall: 0.83 - ETA: 0s - loss: 0.1114 - precision: 0.9287 - recall: 0.83 - ETA: 0s - loss: 0.1112 - precision: 0.9274 - recall: 0.83 - ETA: 0s - loss: 0.1115 - precision: 0.9289 - recall: 0.83 - ETA: 0s - loss: 0.1108 - precision: 0.9273 - recall: 0.83 - 1s 25us/step - loss: 0.1110 - precision: 0.9269 - recall: 0.8370\n",
      "Epoch 7/100\n",
      "23660/23660 [==============================] - 1s 27us/step - loss: 0.1071 - precision: 0.9221 - recall: 0.8485 0s - loss: 0.0945 - precision: 0.9317 - recall: 0.85 - ETA: 0s - loss: 0.0985 - precision: 0.9428 - recall: 0.86 - ETA: 0s - loss: 0.1034 - precision: 0.9319 - recall: 0.85 - ETA: 0s - loss: 0.1083 - precision: 0.9262 - recall: 0.84 - ETA: 0s - loss: 0.1038 - precision: 0.9325 - recall: 0.85 - ETA: 0s - loss: 0.1042 - precision: 0.9326 - recall: 0.85 - ETA: 0s - loss: 0.1048 - precision: 0.9315 - recall: 0.85 - ETA: 0s - loss: 0.1050 - precision: 0.9312 - recall: 0.85 - ETA: 0s - loss: 0.1053 - precision: 0.9291 - recall: 0.85 - ETA: 0s - loss: 0.1064 - precision: 0.9248 - recall: 0.84 - ETA: 0s - loss: 0.1082 - precision: 0.9230 - recall: 0.84 - ETA: 0s - loss: 0.1073 - precision: 0.9237 - recall: 0.84\n",
      "Epoch 8/100\n",
      "23660/23660 [==============================] - 1s 25us/step - loss: 0.1011 - precision: 0.9337 - recall: 0.8562 0s - loss: 0.1011 - precision: 0.9452 - recall:  - ETA: 0s - loss: 0.1015 - precision: 0.9428 - recall: 0.84 - ETA: 0s - loss: 0.1021 - precision: 0.9381 - recall: 0.84 - ETA: 0s - loss: 0.1014 - precision: 0.9368 - recall: 0.84 - ETA: 0s - loss: 0.1012 - precision: 0.9355 - recall: 0.85 - ETA: 0s - loss: 0.1027 - precision: 0.9351 - recall: 0.85 - ETA: 0s - loss: 0.1016 - precision: 0.9339 - recall: 0.85 - ETA: 0s - loss: 0.1010 - precision: 0.9332 - recall: 0.85 - ETA: 0s - loss: 0.1003 - precision: 0.9348 - recall: 0.85\n",
      "Epoch 9/100\n",
      "23660/23660 [==============================] - ETA: 0s - loss: 0.0497 - precision: 0.9524 - recall: 1.00 - ETA: 0s - loss: 0.0892 - precision: 0.9437 - recall: 0.87 - ETA: 0s - loss: 0.0919 - precision: 0.9264 - recall: 0.86 - ETA: 0s - loss: 0.0943 - precision: 0.9276 - recall: 0.86 - ETA: 0s - loss: 0.0952 - precision: 0.9280 - recall: 0.86 - ETA: 0s - loss: 0.0958 - precision: 0.9294 - recall: 0.86 - ETA: 0s - loss: 0.0965 - precision: 0.9256 - recall: 0.86 - ETA: 0s - loss: 0.0975 - precision: 0.9275 - recall: 0.85 - ETA: 0s - loss: 0.0971 - precision: 0.9274 - recall: 0.85 - ETA: 0s - loss: 0.0978 - precision: 0.9274 - recall: 0.86 - ETA: 0s - loss: 0.0977 - precision: 0.9281 - recall: 0.86 - ETA: 0s - loss: 0.0990 - precision: 0.9246 - recall: 0.85 - 1s 26us/step - loss: 0.0992 - precision: 0.9252 - recall: 0.8595\n",
      "Epoch 10/100\n",
      "23660/23660 [==============================] - ETA: 0s - loss: 0.0838 - precision: 0.9404 - recall: 0.86 - ETA: 0s - loss: 0.0868 - precision: 0.9451 - recall: 0.86 - ETA: 0s - loss: 0.0888 - precision: 0.9448 - recall: 0.86 - ETA: 0s - loss: 0.0891 - precision: 0.9423 - recall: 0.86 - ETA: 0s - loss: 0.0931 - precision: 0.9391 - recall: 0.85 - ETA: 0s - loss: 0.0946 - precision: 0.9370 - recall: 0.86 - ETA: 0s - loss: 0.0955 - precision: 0.9338 - recall: 0.86 - ETA: 0s - loss: 0.0954 - precision: 0.9315 - recall: 0.86 - ETA: 0s - loss: 0.0966 - precision: 0.9321 - recall: 0.86 - ETA: 0s - loss: 0.0964 - precision: 0.9304 - recall: 0.86 - ETA: 0s - loss: 0.0964 - precision: 0.9301 - recall: 0.86 - 1s 24us/step - loss: 0.0963 - precision: 0.9305 - recall: 0.8658\n",
      "Epoch 11/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23660/23660 [==============================] - ETA: 0s - loss: 0.0939 - precision: 0.9351 - recall: 0.8681 ETA: 0s - loss: 0.1026 - precision: 0.9356 - recall: 0.84 - ETA: 0s - loss: 0.0995 - precision: 0.9369 - recall: 0.85 - ETA: 0s - loss: 0.0962 - precision: 0.9346 - recall: 0. - ETA: 0s - loss: 0.0956 - precision: 0.9331 - recall: 0.86 - ETA: 0s - loss: 0.0943 - precision: 0.9301 - recall: 0.86 - ETA: 0s - loss: 0.0946 - precision: 0.9305 - recall: 0.86 - ETA: 0s - loss: 0.0942 - precision: 0.9303 - recall: 0.86 - ETA: 0s - loss: 0.0942 - precision: 0.9300 - recall: 0.86 - ETA: 0s - loss: 0.0944 - precision: 0.9288 - recall: 0.86 - ETA: 0s - loss: 0.0945 - precision: 0.9290 - recall: 0.86 - 1s 24us/step - loss: 0.0945 - precision: 0.9292 - recall: 0.8687\n",
      "Epoch 12/100\n",
      "23660/23660 [==============================] - ETA: 0s - loss: 0.1019 - precision: 0.7778 - recall: 0.93 - ETA: 0s - loss: 0.0913 - precision: 0.9177 - recall: 0.88 - ETA: 0s - loss: 0.0900 - precision: 0.9182 - recall: 0.87 - ETA: 0s - loss: 0.0892 - precision: 0.9267 - recall: 0.87 - ETA: 0s - loss: 0.0906 - precision: 0.9319 - recall: 0.87 - ETA: 0s - loss: 0.0923 - precision: 0.9281 - recall: 0.87 - ETA: 0s - loss: 0.0923 - precision: 0.9282 - recall: 0.86 - ETA: 0s - loss: 0.0931 - precision: 0.9249 - recall: 0.87 - ETA: 0s - loss: 0.0938 - precision: 0.9283 - recall: 0.87 - ETA: 0s - loss: 0.0929 - precision: 0.9284 - recall: 0.87 - ETA: 0s - loss: 0.0937 - precision: 0.9281 - recall: 0.87 - ETA: 0s - loss: 0.0923 - precision: 0.9303 - recall: 0.87 - ETA: 0s - loss: 0.0919 - precision: 0.9308 - recall: 0.87 - 1s 27us/step - loss: 0.0922 - precision: 0.9312 - recall: 0.8746\n",
      "Epoch 13/100\n",
      "23660/23660 [==============================] - ETA: 0s - loss: 0.1302 - precision: 0.8750 - recall: 0.87 - ETA: 0s - loss: 0.0863 - precision: 0.9201 - recall: 0.90 - ETA: 0s - loss: 0.0911 - precision: 0.9259 - recall: 0.87 - ETA: 0s - loss: 0.0908 - precision: 0.9232 - recall: 0.87 - ETA: 0s - loss: 0.0911 - precision: 0.9189 - recall: 0.87 - ETA: 0s - loss: 0.0876 - precision: 0.9246 - recall: 0.88 - ETA: 0s - loss: 0.0883 - precision: 0.9265 - recall: 0.87 - ETA: 0s - loss: 0.0876 - precision: 0.9269 - recall: 0.88 - ETA: 0s - loss: 0.0861 - precision: 0.9283 - recall: 0.88 - ETA: 0s - loss: 0.0879 - precision: 0.9292 - recall: 0.88 - ETA: 0s - loss: 0.0902 - precision: 0.9261 - recall: 0.87 - ETA: 0s - loss: 0.0888 - precision: 0.9276 - recall: 0.87 - 1s 26us/step - loss: 0.0898 - precision: 0.9280 - recall: 0.8790\n",
      "Epoch 14/100\n",
      "23660/23660 [==============================] - ETA: 0s - loss: 0.1145 - precision: 0.9143 - recall: 0.94 - ETA: 0s - loss: 0.0972 - precision: 0.9145 - recall: 0.87 - ETA: 0s - loss: 0.0851 - precision: 0.9254 - recall: 0.88 - ETA: 0s - loss: 0.0835 - precision: 0.9256 - recall: 0.87 - ETA: 0s - loss: 0.0823 - precision: 0.9270 - recall: 0.88 - ETA: 0s - loss: 0.0838 - precision: 0.9339 - recall: 0.87 - ETA: 0s - loss: 0.0867 - precision: 0.9322 - recall: 0.87 - ETA: 0s - loss: 0.0880 - precision: 0.9307 - recall: 0.87 - ETA: 0s - loss: 0.0871 - precision: 0.9305 - recall: 0.87 - ETA: 0s - loss: 0.0875 - precision: 0.9311 - recall: 0.87 - ETA: 0s - loss: 0.0870 - precision: 0.9319 - recall: 0.87 - ETA: 0s - loss: 0.0871 - precision: 0.9315 - recall: 0.87 - ETA: 0s - loss: 0.0877 - precision: 0.9306 - recall: 0.87 - 1s 26us/step - loss: 0.0875 - precision: 0.9309 - recall: 0.8779\n",
      "Epoch 15/100\n",
      "23660/23660 [==============================] - 1s 27us/step - loss: 0.0860 - precision: 0.9331 - recall: 0.8826 0s - loss: 0.1038 - precision: 0.9116 - recall: 0.87 - ETA: 0s - loss: 0.0958 - precision: 0.9219 - recall: 0.88 - ETA: 0s - loss: 0.0870 - precision: 0.9327 - recall: 0.88 - ETA: 0s - loss: 0.0871 - precision: 0.9356 - recall: 0.88 - ETA: 0s - loss: 0.0874 - precision: 0.9312 - recall: 0.88 - ETA: 0s - loss: 0.0869 - precision: 0.9329 - recall: 0.88 - ETA: 0s - loss: 0.0861 - precision: 0.9333 - recall: 0.88 - ETA: 0s - loss: 0.0861 - precision: 0.9326 - recall: 0.88 - ETA: 0s - loss: 0.0868 - precision: 0.9291 - recall: 0.88 - ETA: 0s - loss: 0.0854 - precision: 0.9326 - recall: 0.88 - ETA: 0s - loss: 0.0861 - precision: 0.9320 - recall: 0.88 - ETA: 0s - loss: 0.0860 - precision: 0.9330 - recall: 0.88\n",
      "Epoch 16/100\n",
      "23660/23660 [==============================] - ETA: 0s - loss: 0.0856 - precision: 0.9299 - recall: 0.8876 ETA: 0s - loss: 0.0939 - precision: 0.9190 - recall: 0.87 - ETA: 0s - loss: 0.0859 - precision: 0.9342 - recall: 0.88 - ETA: 0s - loss: 0.0850 - precision: 0.9299 - recall: 0.88 - ETA: 0s - loss: 0.0837 - precision: 0.9347 - recall: 0.88 - ETA: 0s - loss: 0.0844 - precision: 0.9312 - recall: 0.88 - ETA: 0s - loss: 0.0844 - precision: 0.9310 - recall: 0. - ETA: 0s - loss: 0.0859 - precision: 0.9301 - recall: 0.88 - ETA: 0s - loss: 0.0871 - precision: 0.9290 - recall: 0.88 - ETA: 0s - loss: 0.0871 - precision: 0.9306 - recall: 0.88 - ETA: 0s - loss: 0.0872 - precision: 0.9280 - recall: 0.88 - ETA: 0s - loss: 0.0869 - precision: 0.9285 - recall: 0.88 - 1s 26us/step - loss: 0.0867 - precision: 0.9287 - recall: 0.8854\n",
      "Epoch 17/100\n",
      "23660/23660 [==============================] - 1s 28us/step - loss: 0.0847 - precision: 0.9307 - recall: 0.8864 0s - loss: 0.0747 - precision: 0.9352 - recall: 0.90 - ETA: 0s - loss: 0.0744 - precision: 0.9367 - recall: 0.90 - ETA: 0s - loss: 0.0791 - precision: 0.9382 - recall: 0. - ETA: 0s - loss: 0.0840 - precision: 0.9305 - recall: 0. - ETA: 0s - loss: 0.0846 - precision: 0.9312 - recall: 0.88 - ETA: 0s - loss: 0.0832 - precision: 0.9344 - recall: 0.88 - ETA: 0s - loss: 0.0834 - precision: 0.9320 - recall: 0.88 - ETA: 0s - loss: 0.0836 - precision: 0.9324 - recall: 0.88 - ETA: 0s - loss: 0.0843 - precision: 0.9305 - recall: 0.88 - ETA: 0s - loss: 0.0853 - precision: 0.9301 - recall: 0.88\n",
      "Epoch 18/100\n",
      "23660/23660 [==============================] - 1s 23us/step - loss: 0.0827 - precision: 0.9344 - recall: 0.8879 0s - loss: 0.0695 - precision: 0.9527 - recall: 0.89 - ETA: 0s - loss: 0.0795 - precision: 0.9393 - recall: 0.88 - ETA: 0s - loss: 0.0823 - precision: 0.9376 - recall: 0.88 - ETA: 0s - loss: 0.0801 - precision: 0.9403 - recall: 0.88 - ETA: 0s - loss: 0.0785 - precision: 0.9418 - recall: 0.89 - ETA: 0s - loss: 0.0787 - precision: 0.9394 - recall: 0.89 - ETA: 0s - loss: 0.0793 - precision: 0.9401 - recall: 0.89 - ETA: 0s - loss: 0.0798 - precision: 0.9353 - recall: 0. - ETA: 0s - loss: 0.0815 - precision: 0.9354 - recall: 0.89\n",
      "Epoch 19/100\n",
      "23660/23660 [==============================] - ETA: 0s - loss: 0.0819 - precision: 0.9342 - recall: 0.8901 ETA: 0s - loss: 0.0861 - precision: 0.9139 - recall: 0.89 - ETA: 0s - loss: 0.0893 - precision: 0.9192 - recall: 0.89 - ETA: 0s - loss: 0.0834 - precision: 0.9337 - recall: 0.89 - ETA: 0s - loss: 0.0837 - precision: 0.9328 - recall: 0.88 - ETA: 0s - loss: 0.0834 - precision: 0.9334 - recall: 0. - ETA: 0s - loss: 0.0818 - precision: 0.9353 - recall: 0.89 - ETA: 0s - loss: 0.0813 - precision: 0.9348 - recall: 0.89 - ETA: 0s - loss: 0.0819 - precision: 0.9329 - recall: 0. - ETA: 0s - loss: 0.0809 - precision: 0.9341 - recall: 0.89 - 1s 24us/step - loss: 0.0811 - precision: 0.9343 - recall: 0.8907\n",
      "Epoch 20/100\n",
      "23660/23660 [==============================] - ETA: 0s - loss: 0.0792 - precision: 0.9388 - recall: 0.9012 ETA: 0s - loss: 0.0810 - precision: 0.9325 - recall:  - ETA: 0s - loss: 0.0805 - precision: 0.9362 - recall: 0.89 - ETA: 0s - loss: 0.0820 - precision: 0.9346 - recall: 0.89 - ETA: 0s - loss: 0.0827 - precision: 0.9343 - recall: 0.89 - ETA: 0s - loss: 0.0820 - precision: 0.9349 - recall: 0.89 - ETA: 0s - loss: 0.0807 - precision: 0.9351 - recall: 0.89 - ETA: 0s - loss: 0.0797 - precision: 0.9345 - recall: 0.89 - ETA: 0s - loss: 0.0797 - precision: 0.9346 - recall: 0.89 - 1s 24us/step - loss: 0.0798 - precision: 0.9358 - recall: 0.8949\n",
      "Epoch 21/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23660/23660 [==============================] - ETA: 0s - loss: 0.0779 - precision: 0.9391 - recall: 0.8956 ETA: 0s - loss: 0.0693 - precision: 0.9485 - recall: 0. - ETA: 0s - loss: 0.0783 - precision: 0.9364 - recall: 0.89 - ETA: 0s - loss: 0.0782 - precision: 0.9373 - recall: 0.89 - ETA: 0s - loss: 0.0781 - precision: 0.9383 - recall: 0. - ETA: 0s - loss: 0.0777 - precision: 0.9400 - recall: 0.89 - ETA: 0s - loss: 0.0798 - precision: 0.9365 - recall: 0.89 - ETA: 0s - loss: 0.0795 - precision: 0.9365 - recall: 0.89 - 1s 21us/step - loss: 0.0800 - precision: 0.9357 - recall: 0.8932\n",
      "Epoch 22/100\n",
      "23660/23660 [==============================] - ETA: 0s - loss: 0.0747 - precision: 0.9474 - recall: 0.85 - ETA: 0s - loss: 0.0760 - precision: 0.9323 - recall: 0.90 - ETA: 0s - loss: 0.0746 - precision: 0.9354 - recall: 0.89 - ETA: 0s - loss: 0.0755 - precision: 0.9363 - recall: 0.89 - ETA: 0s - loss: 0.0737 - precision: 0.9392 - recall: 0.89 - ETA: 0s - loss: 0.0765 - precision: 0.9380 - recall: 0.89 - ETA: 0s - loss: 0.0766 - precision: 0.9374 - recall: 0.89 - ETA: 0s - loss: 0.0758 - precision: 0.9383 - recall: 0.89 - ETA: 0s - loss: 0.0771 - precision: 0.9376 - recall: 0.89 - ETA: 0s - loss: 0.0767 - precision: 0.9369 - recall: 0.89 - ETA: 0s - loss: 0.0773 - precision: 0.9376 - recall: 0.89 - 1s 22us/step - loss: 0.0772 - precision: 0.9378 - recall: 0.8951\n",
      "Epoch 23/100\n",
      "23660/23660 [==============================] - ETA: 0s - loss: 0.0769 - precision: 0.9397 - recall: 0.8999 ETA: 0s - loss: 0.0814 - precision: 0.9315 - recall: 0.90 - ETA: 0s - loss: 0.0790 - precision: 0.9435 - recall: 0.90 - ETA: 0s - loss: 0.0769 - precision: 0.9436 - recall: 0.90 - ETA: 0s - loss: 0.0772 - precision: 0.9447 - recall: 0.90 - ETA: 0s - loss: 0.0747 - precision: 0.9413 - recall: 0.90 - ETA: 0s - loss: 0.0756 - precision: 0.9407 - recall: 0.90 - ETA: 0s - loss: 0.0755 - precision: 0.9408 - recall: 0. - ETA: 0s - loss: 0.0768 - precision: 0.9400 - recall: 0.89 - 1s 22us/step - loss: 0.0767 - precision: 0.9390 - recall: 0.8982\n",
      "Epoch 24/100\n",
      "23660/23660 [==============================] - ETA: 0s - loss: 0.0806 - precision: 0.9058 - recall: 0.89 - ETA: 0s - loss: 0.0760 - precision: 0.9324 - recall: 0.89 - ETA: 0s - loss: 0.0768 - precision: 0.9306 - recall: 0.90 - ETA: 0s - loss: 0.0738 - precision: 0.9377 - recall: 0.90 - ETA: 0s - loss: 0.0749 - precision: 0.9358 - recall: 0.89 - ETA: 0s - loss: 0.0747 - precision: 0.9360 - recall: 0.89 - ETA: 0s - loss: 0.0761 - precision: 0.9358 - recall: 0.90 - ETA: 0s - loss: 0.0756 - precision: 0.9378 - recall: 0.90 - ETA: 0s - loss: 0.0759 - precision: 0.9387 - recall: 0.90 - ETA: 0s - loss: 0.0752 - precision: 0.9382 - recall: 0.90 - 1s 24us/step - loss: 0.0755 - precision: 0.9376 - recall: 0.9024\n",
      "Epoch 25/100\n",
      "23660/23660 [==============================] - ETA: 0s - loss: 0.0789 - precision: 0.8571 - recall: 0.85 - ETA: 0s - loss: 0.0669 - precision: 0.9555 - recall: 0.90 - ETA: 0s - loss: 0.0690 - precision: 0.9487 - recall: 0.90 - ETA: 0s - loss: 0.0686 - precision: 0.9399 - recall: 0.90 - ETA: 0s - loss: 0.0731 - precision: 0.9342 - recall: 0.90 - ETA: 0s - loss: 0.0724 - precision: 0.9383 - recall: 0.90 - ETA: 0s - loss: 0.0720 - precision: 0.9399 - recall: 0.90 - ETA: 0s - loss: 0.0712 - precision: 0.9402 - recall: 0.90 - ETA: 0s - loss: 0.0722 - precision: 0.9390 - recall: 0.90 - ETA: 0s - loss: 0.0728 - precision: 0.9378 - recall: 0.90 - ETA: 0s - loss: 0.0735 - precision: 0.9394 - recall: 0.90 - ETA: 0s - loss: 0.0736 - precision: 0.9391 - recall: 0.90 - 1s 25us/step - loss: 0.0741 - precision: 0.9385 - recall: 0.9028\n",
      "Epoch 26/100\n",
      "23660/23660 [==============================] - ETA: 0s - loss: 0.0881 - precision: 0.8889 - recall: 0.84 - ETA: 0s - loss: 0.0733 - precision: 0.9358 - recall: 0.89 - ETA: 0s - loss: 0.0674 - precision: 0.9418 - recall: 0.90 - ETA: 0s - loss: 0.0671 - precision: 0.9431 - recall: 0.90 - ETA: 0s - loss: 0.0699 - precision: 0.9411 - recall: 0.89 - ETA: 0s - loss: 0.0714 - precision: 0.9359 - recall: 0.90 - ETA: 0s - loss: 0.0720 - precision: 0.9363 - recall: 0.90 - ETA: 0s - loss: 0.0725 - precision: 0.9391 - recall: 0.90 - ETA: 0s - loss: 0.0731 - precision: 0.9395 - recall: 0.90 - ETA: 0s - loss: 0.0730 - precision: 0.9396 - recall: 0.90 - ETA: 0s - loss: 0.0744 - precision: 0.9357 - recall: 0.90 - ETA: 0s - loss: 0.0743 - precision: 0.9388 - recall: 0.90 - ETA: 0s - loss: 0.0741 - precision: 0.9393 - recall: 0.90 - 1s 28us/step - loss: 0.0745 - precision: 0.9366 - recall: 0.9008\n",
      "Epoch 27/100\n",
      "23660/23660 [==============================] - ETA: 0s - loss: 0.0727 - precision: 0.9371 - recall: 0.9037 ETA: 0s - loss: 0.0741 - precision: 0.9324 - recall: 0.89 - ETA: 0s - loss: 0.0733 - precision: 0.9339 - recall: 0.90 - ETA: 0s - loss: 0.0718 - precision: 0.9424 - recall: 0.90 - ETA: 0s - loss: 0.0738 - precision: 0.9363 - recall:  - ETA: 0s - loss: 0.0730 - precision: 0.9351 - recall: 0. - ETA: 0s - loss: 0.0732 - precision: 0.9364 - recall: 0.90 - ETA: 0s - loss: 0.0736 - precision: 0.9372 - recall: 0.90 - 1s 23us/step - loss: 0.0734 - precision: 0.9374 - recall: 0.9026\n",
      "Epoch 28/100\n",
      "23660/23660 [==============================] - ETA: 0s - loss: 0.0712 - precision: 0.9404 - recall: 0.9067 ETA: 0s - loss: 0.0621 - precision: 0.9420 - recall: 0.90 - ETA: 0s - loss: 0.0642 - precision: 0.9432 - recall: 0.91 - ETA: 0s - loss: 0.0689 - precision: 0.9485 - recall: 0.89 - ETA: 0s - loss: 0.0697 - precision: 0.9467 - recall: 0.90 - ETA: 0s - loss: 0.0679 - precision: 0.9455 - recall: 0.90 - ETA: 0s - loss: 0.0692 - precision: 0.9443 - recall: 0.90 - ETA: 0s - loss: 0.0706 - precision: 0.9431 - recall:  - ETA: 0s - loss: 0.0709 - precision: 0.9408 - recall: 0.90 - 1s 23us/step - loss: 0.0708 - precision: 0.9410 - recall: 0.9057\n",
      "Epoch 29/100\n",
      "23660/23660 [==============================] - 1s 26us/step - loss: 0.0718 - precision: 0.9409 - recall: 0.9055 0s - loss: 0.0688 - precision: 0.9487 - recall: 0.89 - ETA: 0s - loss: 0.0692 - precision: 0.9430 - recall: 0.91 - ETA: 0s - loss: 0.0662 - precision: 0.9443 - recall: 0.90 - ETA: 0s - loss: 0.0664 - precision: 0.9477 - recall: 0.91 - ETA: 0s - loss: 0.0646 - precision: 0.9523 - recall: 0.91 - ETA: 0s - loss: 0.0670 - precision: 0.9469 - recall: 0.91 - ETA: 0s - loss: 0.0711 - precision: 0.9418 - recall: 0.91 - ETA: 0s - loss: 0.0706 - precision: 0.9411 - recall: 0.90 - ETA: 0s - loss: 0.0717 - precision: 0.9404 - recall: 0.90 - ETA: 0s - loss: 0.0720 - precision: 0.9400 - recall: 0.\n",
      "Epoch 30/100\n",
      "23660/23660 [==============================] - ETA: 0s - loss: 0.0528 - precision: 1.0000 - recall: 0.94 - ETA: 0s - loss: 0.0755 - precision: 0.9253 - recall: 0.88 - ETA: 0s - loss: 0.0638 - precision: 0.9410 - recall: 0.91 - ETA: 0s - loss: 0.0638 - precision: 0.9478 - recall: 0.91 - ETA: 0s - loss: 0.0666 - precision: 0.9468 - recall: 0.90 - ETA: 0s - loss: 0.0678 - precision: 0.9497 - recall: 0.90 - ETA: 0s - loss: 0.0680 - precision: 0.9458 - recall: 0.90 - ETA: 0s - loss: 0.0668 - precision: 0.9474 - recall: 0.91 - ETA: 0s - loss: 0.0678 - precision: 0.9455 - recall: 0.90 - ETA: 0s - loss: 0.0684 - precision: 0.9437 - recall: 0.90 - ETA: 0s - loss: 0.0680 - precision: 0.9444 - recall: 0.90 - ETA: 0s - loss: 0.0688 - precision: 0.9445 - recall: 0.90 - ETA: 0s - loss: 0.0690 - precision: 0.9440 - recall: 0.91 - 1s 27us/step - loss: 0.0689 - precision: 0.9459 - recall: 0.9099\n",
      "Epoch 31/100\n",
      "23660/23660 [==============================] - 1s 24us/step - loss: 0.0689 - precision: 0.9442 - recall: 0.9101 0s - loss: 0.0724 - precision: 0.9271 - recall: 0.90 - ETA: 0s - loss: 0.0694 - precision: 0.9358 - recall: 0.91 - ETA: 0s - loss: 0.0710 - precision: 0.9407 - recall: 0.90 - ETA: 0s - loss: 0.0706 - precision: 0.9391 - recall: 0. - ETA: 0s - loss: 0.0690 - precision: 0.9422 - recall: 0. - ETA: 0s - loss: 0.0706 - precision: 0.9413 - recall: 0.\n",
      "Epoch 32/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23660/23660 [==============================] - 1s 26us/step - loss: 0.0683 - precision: 0.9447 - recall: 0.9107 0s - loss: 0.0689 - precision: 0.9455 - recall: 0.90 - ETA: 0s - loss: 0.0668 - precision: 0.9482 - recall: 0.92 - ETA: 0s - loss: 0.0664 - precision: 0.9522 - recall: 0.91 - ETA: 0s - loss: 0.0664 - precision: 0.9472 - recall: 0. - ETA: 0s - loss: 0.0684 - precision: 0.9468 - recall: 0.90 - ETA: 0s - loss: 0.0673 - precision: 0.9469 - recall: 0.91 - ETA: 0s - loss: 0.0683 - precision: 0.9455 - recall: 0.91 - ETA: 0s - loss: 0.0687 - precision: 0.9450 - recall: 0.91 - ETA: 0s - loss: 0.0691 - precision: 0.9436 - recall: 0.\n",
      "Epoch 33/100\n",
      "23660/23660 [==============================] - 1s 25us/step - loss: 0.0675 - precision: 0.9470 - recall: 0.9118 0s - loss: 0.0620 - precision: 0.9318 - recall: 0.91 - ETA: 0s - loss: 0.0629 - precision: 0.9528 - recall: 0.90 - ETA: 0s - loss: 0.0625 - precision: 0.9493 - recall: 0. - ETA: 0s - loss: 0.0667 - precision: 0.9466 - recall: 0.91 - ETA: 0s - loss: 0.0674 - precision: 0.9451 - recall: 0.91 - ETA: 0s - loss: 0.0677 - precision: 0.9480 - recall: 0.91 - ETA: 0s - loss: 0.0682 - precision: 0.9464 - recall: 0.91 - ETA: 0s - loss: 0.0678 - precision: 0.9475 - recall: 0.90 - ETA: 0s - loss: 0.0669 - precision: 0.9469 - recall: 0.\n",
      "Epoch 34/100\n",
      "23660/23660 [==============================] - ETA: 0s - loss: 0.0387 - precision: 1.0000 - recall: 1.00 - ETA: 0s - loss: 0.0593 - precision: 0.9592 - recall: 0.94 - ETA: 0s - loss: 0.0644 - precision: 0.9489 - recall: 0.92 - ETA: 0s - loss: 0.0646 - precision: 0.9421 - recall: 0.92 - ETA: 0s - loss: 0.0656 - precision: 0.9416 - recall: 0.91 - ETA: 0s - loss: 0.0653 - precision: 0.9455 - recall: 0.91 - ETA: 0s - loss: 0.0629 - precision: 0.9509 - recall: 0.91 - ETA: 0s - loss: 0.0644 - precision: 0.9491 - recall: 0.91 - ETA: 0s - loss: 0.0654 - precision: 0.9466 - recall: 0.91 - ETA: 0s - loss: 0.0647 - precision: 0.9470 - recall: 0.91 - ETA: 0s - loss: 0.0665 - precision: 0.9461 - recall: 0.91 - ETA: 0s - loss: 0.0674 - precision: 0.9435 - recall: 0.91 - ETA: 0s - loss: 0.0676 - precision: 0.9440 - recall: 0.91 - 1s 28us/step - loss: 0.0672 - precision: 0.9447 - recall: 0.9109\n",
      "Epoch 35/100\n",
      "23660/23660 [==============================] - ETA: 0s - loss: 0.0617 - precision: 1.0000 - recall: 0.90 - ETA: 0s - loss: 0.0633 - precision: 0.9633 - recall: 0.91 - ETA: 0s - loss: 0.0638 - precision: 0.9496 - recall: 0.91 - ETA: 0s - loss: 0.0647 - precision: 0.9480 - recall: 0.91 - ETA: 0s - loss: 0.0646 - precision: 0.9453 - recall: 0.91 - ETA: 0s - loss: 0.0633 - precision: 0.9486 - recall: 0.91 - ETA: 0s - loss: 0.0630 - precision: 0.9483 - recall: 0.91 - ETA: 0s - loss: 0.0633 - precision: 0.9496 - recall: 0.91 - ETA: 0s - loss: 0.0629 - precision: 0.9510 - recall: 0.91 - ETA: 0s - loss: 0.0639 - precision: 0.9490 - recall: 0.91 - ETA: 0s - loss: 0.0653 - precision: 0.9483 - recall: 0.91 - ETA: 0s - loss: 0.0656 - precision: 0.9477 - recall: 0.91 - ETA: 0s - loss: 0.0664 - precision: 0.9450 - recall: 0.91 - 1s 27us/step - loss: 0.0665 - precision: 0.9450 - recall: 0.9147\n",
      "Epoch 36/100\n",
      "23660/23660 [==============================] - ETA: 0s - loss: 0.0647 - precision: 0.9540 - recall: 0.9157 ETA: 0s - loss: 0.0788 - precision: 0.9402 - recall: 0.89 - ETA: 0s - loss: 0.0662 - precision: 0.9553 - recall: 0.91 - ETA: 0s - loss: 0.0633 - precision: 0.9587 - recall: 0.92 - ETA: 0s - loss: 0.0607 - precision: 0.9615 - recall: 0.92 - ETA: 0s - loss: 0.0600 - precision: 0.9599 - recall: 0.92 - ETA: 0s - loss: 0.0631 - precision: 0.9566 - recall: 0.91 - ETA: 0s - loss: 0.0646 - precision: 0.9540 - recall: 0.91 - ETA: 0s - loss: 0.0647 - precision: 0.9526 - recall: 0.91 - ETA: 0s - loss: 0.0643 - precision: 0.9546 - recall: 0.91 - ETA: 0s - loss: 0.0644 - precision: 0.9548 - recall: 0. - 1s 26us/step - loss: 0.0657 - precision: 0.9515 - recall: 0.9151\n",
      "Epoch 37/100\n",
      "23660/23660 [==============================] - ETA: 0s - loss: 0.0648 - precision: 0.9483 - recall: 0.9188 ETA: 0s - loss: 0.0560 - precision: 0.9515 - recall: 0.93 - ETA: 0s - loss: 0.0564 - precision: 0.9575 - recall: 0.92 - ETA: 0s - loss: 0.0568 - precision: 0.9557 - recall: 0. - ETA: 0s - loss: 0.0591 - precision: 0.9507 - recall: 0.92 - ETA: 0s - loss: 0.0588 - precision: 0.9526 - recall: 0.92 - ETA: 0s - loss: 0.0619 - precision: 0.9508 - recall: 0.92 - ETA: 0s - loss: 0.0630 - precision: 0.9494 - recall: 0.92 - ETA: 0s - loss: 0.0641 - precision: 0.9486 - recall: 0. - ETA: 0s - loss: 0.0645 - precision: 0.9490 - recall: 0.92 - 1s 25us/step - loss: 0.0645 - precision: 0.9497 - recall: 0.9193\n",
      "Epoch 38/100\n",
      "23660/23660 [==============================] - ETA: 0s - loss: 0.0580 - precision: 0.9520 - recall: 0.94 - ETA: 0s - loss: 0.0563 - precision: 0.9498 - recall: 0.93 - ETA: 0s - loss: 0.0570 - precision: 0.9460 - recall: 0.93 - ETA: 0s - loss: 0.0608 - precision: 0.9449 - recall: 0.92 - ETA: 0s - loss: 0.0602 - precision: 0.9476 - recall: 0.92 - ETA: 0s - loss: 0.0620 - precision: 0.9458 - recall: 0.92 - ETA: 0s - loss: 0.0632 - precision: 0.9449 - recall: 0.92 - ETA: 0s - loss: 0.0632 - precision: 0.9447 - recall: 0.92 - ETA: 0s - loss: 0.0633 - precision: 0.9463 - recall: 0.91 - ETA: 0s - loss: 0.0640 - precision: 0.9456 - recall: 0.91 - 1s 23us/step - loss: 0.0647 - precision: 0.9457 - recall: 0.9175\n",
      "Epoch 39/100\n",
      "23660/23660 [==============================] - 1s 26us/step - loss: 0.0630 - precision: 0.9490 - recall: 0.9208 0s - loss: 0.0580 - precision: 0.9541 - recall: 0.93 - ETA: 0s - loss: 0.0621 - precision: 0.9486 - recall: 0.92 - ETA: 0s - loss: 0.0607 - precision: 0.9524 - recall: 0.92 - ETA: 0s - loss: 0.0608 - precision: 0.9521 - recall: 0.92 - ETA: 0s - loss: 0.0610 - precision: 0.9513 - recall: 0.92 - ETA: 0s - loss: 0.0609 - precision: 0.9509 - recall: 0.92 - ETA: 0s - loss: 0.0598 - precision: 0.9526 - recall: 0.92 - ETA: 0s - loss: 0.0601 - precision: 0.9524 - recall: 0.92 - ETA: 0s - loss: 0.0599 - precision: 0.9549 - recall: 0.92 - ETA: 0s - loss: 0.0608 - precision: 0.9526 - recall: 0.92 - ETA: 0s - loss: 0.0627 - precision: 0.9491 - recall: 0.91\n",
      "Epoch 40/100\n",
      "23660/23660 [==============================] - ETA: 0s - loss: 0.0343 - precision: 0.9545 - recall: 1.00 - ETA: 0s - loss: 0.0646 - precision: 0.9572 - recall: 0.91 - ETA: 0s - loss: 0.0646 - precision: 0.9531 - recall: 0.92 - ETA: 0s - loss: 0.0639 - precision: 0.9566 - recall: 0.92 - ETA: 0s - loss: 0.0637 - precision: 0.9523 - recall: 0.92 - ETA: 0s - loss: 0.0629 - precision: 0.9532 - recall: 0.92 - ETA: 0s - loss: 0.0652 - precision: 0.9492 - recall: 0.91 - ETA: 0s - loss: 0.0642 - precision: 0.9481 - recall: 0.91 - ETA: 0s - loss: 0.0632 - precision: 0.9486 - recall: 0.91 - ETA: 0s - loss: 0.0633 - precision: 0.9476 - recall: 0.91 - ETA: 0s - loss: 0.0634 - precision: 0.9480 - recall: 0.91 - ETA: 0s - loss: 0.0635 - precision: 0.9485 - recall: 0.91 - 1s 25us/step - loss: 0.0633 - precision: 0.9478 - recall: 0.9182\n",
      "Epoch 41/100\n",
      "23660/23660 [==============================] - ETA: 0s - loss: 0.1248 - precision: 0.9412 - recall: 0.84 - ETA: 0s - loss: 0.0642 - precision: 0.9369 - recall: 0.91 - ETA: 0s - loss: 0.0606 - precision: 0.9535 - recall: 0.92 - ETA: 0s - loss: 0.0582 - precision: 0.9558 - recall: 0.91 - ETA: 0s - loss: 0.0576 - precision: 0.9535 - recall: 0.92 - ETA: 0s - loss: 0.0597 - precision: 0.9514 - recall: 0.91 - ETA: 0s - loss: 0.0604 - precision: 0.9532 - recall: 0.91 - ETA: 0s - loss: 0.0607 - precision: 0.9532 - recall: 0.92 - ETA: 0s - loss: 0.0617 - precision: 0.9499 - recall: 0.92 - ETA: 0s - loss: 0.0614 - precision: 0.9505 - recall: 0.91 - ETA: 0s - loss: 0.0607 - precision: 0.9522 - recall: 0.92 - ETA: 0s - loss: 0.0623 - precision: 0.9504 - recall: 0.91 - 1s 25us/step - loss: 0.0624 - precision: 0.9490 - recall: 0.9199\n",
      "Epoch 42/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23660/23660 [==============================] - 1s 22us/step - loss: 0.0609 - precision: 0.9531 - recall: 0.9226 0s - loss: 0.0553 - precision: 0.9663 - recall: 0.93 - ETA: 0s - loss: 0.0549 - precision: 0.9661 - recall: 0.93 - ETA: 0s - loss: 0.0550 - precision: 0.9638 - recall: 0.93 - ETA: 0s - loss: 0.0553 - precision: 0.9672 - recall: 0.93 - ETA: 0s - loss: 0.0569 - precision: 0.9629 - recall - ETA: 0s - loss: 0.0594 - precision: 0.9564 - recall: 0.92 - ETA: 0s - loss: 0.0608 - precision: 0.9532 - recall: 0.92\n",
      "Epoch 43/100\n",
      "23660/23660 [==============================] - ETA: 0s - loss: 0.0624 - precision: 0.9331 - recall: 0.92 - ETA: 0s - loss: 0.0628 - precision: 0.9457 - recall: 0.91 - ETA: 0s - loss: 0.0612 - precision: 0.9473 - recall: 0.91 - ETA: 0s - loss: 0.0601 - precision: 0.9496 - recall: 0.92 - ETA: 0s - loss: 0.0589 - precision: 0.9524 - recall: 0.92 - ETA: 0s - loss: 0.0613 - precision: 0.9499 - recall: 0.92 - ETA: 0s - loss: 0.0607 - precision: 0.9497 - recall: 0.92 - ETA: 0s - loss: 0.0613 - precision: 0.9509 - recall: 0.92 - ETA: 0s - loss: 0.0622 - precision: 0.9483 - recall: 0.91 - ETA: 0s - loss: 0.0610 - precision: 0.9489 - recall: 0.92 - 1s 23us/step - loss: 0.0610 - precision: 0.9493 - recall: 0.9208\n",
      "Epoch 44/100\n",
      "23660/23660 [==============================] - 1s 24us/step - loss: 0.0605 - precision: 0.9518 - recall: 0.9215 0s - loss: 0.0554 - precision: 0.9419 - recall: 0.92 - ETA: 0s - loss: 0.0570 - precision: 0.9514 - recall: 0.92 - ETA: 0s - loss: 0.0553 - precision: 0.9563 - recall: 0.93 - ETA: 0s - loss: 0.0556 - precision: 0.9564 - recall: 0.92 - ETA: 0s - loss: 0.0550 - precision: 0.9574 - recall: 0.92 - ETA: 0s - loss: 0.0582 - precision: 0.9558 - recall: 0.92 - ETA: 0s - loss: 0.0589 - precision: 0.9549 - recall: 0.92 - ETA: 0s - loss: 0.0596 - precision: 0.9536 - recall: 0.92 - ETA: 0s - loss: 0.0598 - precision: 0.9520 - recall: 0.\n",
      "Epoch 45/100\n",
      "23660/23660 [==============================] - ETA: 0s - loss: 0.0593 - precision: 0.9515 - recall: 0.9218 ETA: 0s - loss: 0.0540 - precision: 0.9513 - recall: 0.94 - ETA: 0s - loss: 0.0565 - precision: 0.9550 - recall: 0.93 - ETA: 0s - loss: 0.0567 - precision: 0.9527 - recall: 0.93 - ETA: 0s - loss: 0.0566 - precision: 0.9513 - recall: 0. - ETA: 0s - loss: 0.0568 - precision: 0.9525 - recall:  - ETA: 0s - loss: 0.0584 - precision: 0.9518 - recall: 0.92 - 1s 21us/step - loss: 0.0590 - precision: 0.9505 - recall: 0.9239\n",
      "Epoch 46/100\n",
      "23660/23660 [==============================] - ETA: 0s - loss: 0.0577 - precision: 0.9537 - recall: 0.9268 ETA: 0s - loss: 0.0511 - precision: 0.9603 - recall: 0.93 - ETA: 0s - loss: 0.0543 - precision: 0.9519 - recall: 0.93 - ETA: 0s - loss: 0.0524 - precision: 0.9594 - recall: 0.93 - ETA: 0s - loss: 0.0533 - precision: 0.9584 - recall: 0.92 - ETA: 0s - loss: 0.0564 - precision: 0.9550 - recall: 0.92 - ETA: 0s - loss: 0.0565 - precision: 0.9547 - recall: 0.92 - ETA: 0s - loss: 0.0574 - precision: 0.9540 - recall:  - 1s 22us/step - loss: 0.0590 - precision: 0.9518 - recall: 0.9250\n",
      "Epoch 47/100\n",
      "23660/23660 [==============================] - ETA: 0s - loss: 0.0603 - precision: 0.9602 - recall: 0.93 - ETA: 0s - loss: 0.0591 - precision: 0.9578 - recall: 0.92 - ETA: 0s - loss: 0.0610 - precision: 0.9506 - recall: 0.92 - ETA: 0s - loss: 0.0581 - precision: 0.9537 - recall: 0.92 - ETA: 0s - loss: 0.0585 - precision: 0.9516 - recall: 0.92 - ETA: 0s - loss: 0.0587 - precision: 0.9482 - recall: 0.92 - ETA: 0s - loss: 0.0578 - precision: 0.9515 - recall: 0.92 - ETA: 0s - loss: 0.0579 - precision: 0.9503 - recall: 0.92 - 0s 21us/step - loss: 0.0585 - precision: 0.9499 - recall: 0.9276\n",
      "Epoch 48/100\n",
      "23660/23660 [==============================] - 0s 19us/step - loss: 0.0585 - precision: 0.9536 - recall: 0.9239 0s - loss: 0.0659 - precision: 0.9443 - recall: 0.90 - ETA: 0s - loss: 0.0613 - precision: 0.9489 - recall: 0.91 - ETA: 0s - loss: 0.0580 - precision: 0.9541 - recall: 0.92 - ETA: 0s - loss: 0.0567 - precision: 0.9568 - recall: 0.92 - ETA: 0s - loss: 0.0572 - precision: 0.9545 - recall: 0. - ETA: 0s - loss: 0.0571 - precision: 0.9558 - recall: 0.92 - ETA: 0s - loss: 0.0591 - precision: 0.9533 - recall: 0.92\n",
      "Epoch 49/100\n",
      "23660/23660 [==============================] - 0s 18us/step - loss: 0.0575 - precision: 0.9506 - recall: 0.9248 0s - loss: 0.0575 - precision: 0.9310 - recall: 0.94 - ETA: 0s - loss: 0.0579 - precision: 0.9460 - recall:  - ETA: 0s - loss: 0.0569 - precision: 0.9464 - recall: 0.92 - ETA: 0s - loss: 0.0575 - precision: 0.9474 - recall: 0. - ETA: 0s - loss: 0.0573 - precision: 0.9504 - recall: 0.92\n",
      "Epoch 50/100\n",
      "23660/23660 [==============================] - 0s 18us/step - loss: 0.0568 - precision: 0.9534 - recall: 0.9265 0s - loss: 0.0509 - precision: 0.9520 - recall: 0. - ETA: 0s - loss: 0.0506 - precision: 0.9553 - recall: 0.92 - ETA: 0s - loss: 0.0531 - precision: 0.9536 - recall: 0.93 - ETA: 0s - loss: 0.0548 - precision: 0.9530 - recall: 0.93 - ETA: 0s - loss: 0.0552 - precision: 0.9545 - recall: \n",
      "Epoch 51/100\n",
      "23660/23660 [==============================] - 0s 18us/step - loss: 0.0565 - precision: 0.9538 - recall: 0.9298 0s - loss: 0.0459 - precision: 0.9615 - recall: 0.94 - ETA: 0s - loss: 0.0526 - precision: 0.9566 - recall: 0. - ETA: 0s - loss: 0.0524 - precision: 0.9561 - reca\n",
      "Epoch 52/100\n",
      "23660/23660 [==============================] - 0s 17us/step - loss: 0.0565 - precision: 0.9523 - recall: 0.9287 0s - loss: 0.0494 - precision: 0.9567 - recall: 0.93 - ETA: 0s - loss: 0.0505 - precision: 0.9566 - recall: 0.93 - ETA: 0s - loss: 0.0528 - precision: 0.9553 - recall: 0. - ETA: 0s - loss: 0.0556 - precision: 0.9533 - recall: 0.93 - ETA: 0s - loss: 0.0564 - precision: 0.9521 - recall: 0.\n",
      "Epoch 53/100\n",
      "23660/23660 [==============================] - 0s 18us/step - loss: 0.0548 - precision: 0.9521 - recall: 0.9342 0s - loss: 0.0503 - precision: 0.9622 - \n",
      "Epoch 54/100\n",
      "23660/23660 [==============================] - ETA: 0s - loss: 0.0548 - precision: 1.0000 - recall: 0.95 - ETA: 0s - loss: 0.0520 - precision: 0.9520 - recall: 0.92 - ETA: 0s - loss: 0.0519 - precision: 0.9512 - recall: 0.93 - ETA: 0s - loss: 0.0521 - precision: 0.9552 - recall: 0.93 - ETA: 0s - loss: 0.0558 - precision: 0.9552 - recall: 0.92 - ETA: 0s - loss: 0.0549 - precision: 0.9531 - recall: 0.93 - ETA: 0s - loss: 0.0537 - precision: 0.9545 - recall: 0.93 - ETA: 0s - loss: 0.0553 - precision: 0.9510 - recall: 0.92 - ETA: 0s - loss: 0.0557 - precision: 0.9509 - recall: 0.93 - 0s 18us/step - loss: 0.0559 - precision: 0.9509 - recall: 0.9296\n",
      "Epoch 55/100\n",
      "23660/23660 [==============================] - ETA: 0s - loss: 0.0413 - precision: 1.0000 - recall: 0.93 - ETA: 0s - loss: 0.0452 - precision: 0.9719 - recall: 0.93 - ETA: 0s - loss: 0.0489 - precision: 0.9709 - recall: 0.93 - ETA: 0s - loss: 0.0493 - precision: 0.9663 - recall: 0.93 - ETA: 0s - loss: 0.0509 - precision: 0.9630 - recall: 0.93 - ETA: 0s - loss: 0.0525 - precision: 0.9614 - recall: 0.93 - ETA: 0s - loss: 0.0528 - precision: 0.9586 - recall: 0.93 - ETA: 0s - loss: 0.0537 - precision: 0.9565 - recall: 0.93 - ETA: 0s - loss: 0.0542 - precision: 0.9561 - recall: 0.93 - 0s 18us/step - loss: 0.0542 - precision: 0.9559 - recall: 0.9316\n",
      "Epoch 56/100\n",
      "23660/23660 [==============================] - ETA: 0s - loss: 0.0495 - precision: 0.9672 - recall: 0.93 - ETA: 0s - loss: 0.0493 - precision: 0.9620 - recall: 0.93 - ETA: 0s - loss: 0.0506 - precision: 0.9614 - recall: 0.93 - ETA: 0s - loss: 0.0513 - precision: 0.9617 - recall: 0.93 - ETA: 0s - loss: 0.0525 - precision: 0.9598 - recall: 0.93 - ETA: 0s - loss: 0.0537 - precision: 0.9565 - recall: 0.93 - ETA: 0s - loss: 0.0539 - precision: 0.9568 - recall: 0.93 - ETA: 0s - loss: 0.0539 - precision: 0.9580 - recall: 0.93 - 0s 18us/step - loss: 0.0542 - precision: 0.9560 - recall: 0.9312\n",
      "Epoch 57/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23660/23660 [==============================] - ETA: 0s - loss: 0.0542 - precision: 0.9613 - recall: 0.9303 ETA: 0s - loss: 0.0499 - precision: 0.9539 - recall: 0.94 - ETA: 0s - loss: 0.0489 - precision: 0.9646 - recall: 0.93 - ETA: 0s - loss: 0.0480 - precision: 0.9658 - recall: 0.94 - ETA: 0s - loss: 0.0503 - precision: 0.9620 - recall: 0.93 - ETA: 0s - loss: 0.0537 - precision: 0.9614 - recall: 0. - ETA: 0s - loss: 0.0548 - precision: 0.9604 - recall: 0.93 - ETA: 0s - loss: 0.0543 - precision: 0.9583 - recall: 0.92 - ETA: 0s - loss: 0.0548 - precision: 0.9567 - recall: 0.93 - 0s 21us/step - loss: 0.0541 - precision: 0.9569 - recall: 0.9316\n",
      "Epoch 58/100\n",
      "23660/23660 [==============================] - 1s 21us/step - loss: 0.0544 - precision: 0.9512 - recall: 0.9305 0s - loss: 0.0595 - precision: 0.9344 - recall: 0.90 - ETA: 0s - loss: 0.0553 - precision: 0.9440 - recall: 0.92 - ETA: 0s - loss: 0.0529 - precision: 0.9516 - recall: 0.92 - ETA: 0s - loss: 0.0533 - precision: 0.9519 - recall: 0.92 - ETA: 0s - loss: 0.0528 - precision: 0.9508 - recall: 0.92 - ETA: 0s - loss: 0.0543 - precision: 0.9508 - recall: 0.92 - ETA: 0s - loss: 0.0531 - precision: 0.9521 - recall: 0.92 - ETA: 0s - loss: 0.0532 - precision: 0.9498 - recall: 0.93 - ETA: 0s - loss: 0.0541 - precision: 0.9501 - recall: 0.93\n",
      "Epoch 59/100\n",
      "23660/23660 [==============================] - 1s 21us/step - loss: 0.0514 - precision: 0.9586 - recall: 0.9358 0s - loss: 0.0499 - precision: 0.9559 - recall: 0.94 - ETA: 0s - loss: 0.0491 - precision: 0.9593 - recall: 0.94 - ETA: 0s - loss: 0.0497 - precision: 0.9581 - recall: 0.94 - ETA: 0s - loss: 0.0479 - precision: 0.9617 - recall: 0.94 - ETA: 0s - loss: 0.0504 - precision: 0.9590 - recall: 0.94 - ETA: 0s - loss: 0.0505 - precision: 0.9584 - recall: 0.94 - ETA: 0s - loss: 0.0520 - precision: 0.9577 - recall: 0.93 - ETA: 0s - loss: 0.0513 - precision: 0.9589 - recall: 0.93 - ETA: 0s - loss: 0.0513 - precision: 0.9589 - recall: 0.93\n",
      "Epoch 60/100\n",
      "23660/23660 [==============================] - ETA: 0s - loss: 0.0490 - precision: 0.9600 - recall: 0.9396 ETA: 0s - loss: 0.0516 - precision: 0.9500 - recall: 0. - ETA: 0s - loss: 0.0494 - precision: 0.9603 - recall: 0.93 - ETA: 0s - loss: 0.0514 - precision: 0.9613 - recall: 0.93 - ETA: 0s - loss: 0.0510 - precision: 0.9589 - recall: 0.93 - ETA: 0s - loss: 0.0513 - precision: 0.9555 - recall: 0.93 - ETA: 0s - loss: 0.0518 - precision: 0.9530 - recall: 0.93 - ETA: 0s - loss: 0.0522 - precision: 0.9517 - recall: 0.93 - ETA: 0s - loss: 0.0526 - precision: 0.9519 - recall: 0.93 - ETA: 0s - loss: 0.0527 - precision: 0.9540 - recall: 0.93 - ETA: 0s - loss: 0.0530 - precision: 0.9526 - recall: 0.93 - 1s 25us/step - loss: 0.0531 - precision: 0.9530 - recall: 0.9329\n",
      "Epoch 61/100\n",
      "23660/23660 [==============================] - 0s 21us/step - loss: 0.0513 - precision: 0.9580 - recall: 0.9382 0s - loss: 0.0634 - precision: 0.9409 - recall: 0.91 - ETA: 0s - loss: 0.0520 - precision: 0.9579 - recall: 0.93 - ETA: 0s - loss: 0.0529 - precision: 0.9606 - \n",
      "Epoch 62/100\n",
      "23660/23660 [==============================] - ETA: 0s - loss: 0.0383 - precision: 1.0000 - recall: 0.95 - ETA: 0s - loss: 0.0507 - precision: 0.9682 - recall: 0.94 - ETA: 0s - loss: 0.0502 - precision: 0.9654 - recall: 0.94 - ETA: 0s - loss: 0.0503 - precision: 0.9620 - recall: 0.94 - ETA: 0s - loss: 0.0479 - precision: 0.9638 - recall: 0.94 - ETA: 0s - loss: 0.0484 - precision: 0.9634 - recall: 0.94 - ETA: 0s - loss: 0.0493 - precision: 0.9617 - recall: 0.94 - ETA: 0s - loss: 0.0505 - precision: 0.9596 - recall: 0.93 - ETA: 0s - loss: 0.0506 - precision: 0.9580 - recall: 0.93 - ETA: 0s - loss: 0.0514 - precision: 0.9562 - recall: 0.93 - ETA: 0s - loss: 0.0524 - precision: 0.9558 - recall: 0.93 - ETA: 0s - loss: 0.0518 - precision: 0.9572 - recall: 0.93 - 1s 24us/step - loss: 0.0518 - precision: 0.9572 - recall: 0.9386\n",
      "Epoch 63/100\n",
      "23660/23660 [==============================] - 1s 26us/step - loss: 0.0510 - precision: 0.9574 - recall: 0.9364 0s - loss: 0.0402 - precision: 0.9719 - recall: 0.95 - ETA: 0s - loss: 0.0423 - precision: 0.9641 - recall: 0.94 - ETA: 0s - loss: 0.0448 - precision: 0.9635 - recall: 0.93 - ETA: 0s - loss: 0.0471 - precision: 0.9589 - recall: 0.93 - ETA: 0s - loss: 0.0481 - precision: 0.9562 - recall: 0.93 - ETA: 0s - loss: 0.0476 - precision: 0.9587 - recall: 0.93 - ETA: 0s - loss: 0.0495 - precision: 0.9587 - recall: 0.93 - ETA: 0s - loss: 0.0489 - precision: 0.9587 - recall: 0.93 - ETA: 0s - loss: 0.0497 - precision: 0.9580 - recall: 0.93 - ETA: 0s - loss: 0.0503 - precision: 0.9568 - recall: 0.93 - ETA: 0s - loss: 0.0514 - precision: 0.9557 - recall: 0.93\n",
      "Epoch 64/100\n",
      "23660/23660 [==============================] - ETA: 0s - loss: 0.0494 - precision: 0.9569 - recall: 0.9396 ETA: 0s - loss: 0.0450 - precision: 0.9618 - recall: 0.93 - ETA: 0s - loss: 0.0466 - precision: 0.9608 - recall: 0.94 - ETA: 0s - loss: 0.0464 - precision: 0.9625 - recall:  - ETA: 0s - loss: 0.0465 - precision: 0.9607 - recall: 0.94 - ETA: 0s - loss: 0.0483 - precision: 0.9604 - recall: 0. - ETA: 0s - loss: 0.0495 - precision: 0.9563 - recall: 0.93 - 0s 21us/step - loss: 0.0499 - precision: 0.9567 - recall: 0.9384\n",
      "Epoch 65/100\n",
      "23660/23660 [==============================] - 1s 26us/step - loss: 0.0494 - precision: 0.9596 - recall: 0.9362 0s - loss: 0.0436 - precision: 0.9624 - recall: 0.95 - ETA: 0s - loss: 0.0448 - precision: 0.9709 - recall: 0.94 - ETA: 0s - loss: 0.0449 - precision: 0.9689 - recall: 0. - ETA: 0s - loss: 0.0457 - precision: 0.9689 - recall: 0.94 - ETA: 0s - loss: 0.0474 - precision: 0.9660 - recall: 0.94 - ETA: 0s - loss: 0.0475 - precision: 0.9654 - recall: 0.93 - ETA: 0s - loss: 0.0476 - precision: 0.9636 - recall: 0.93 - ETA: 0s - loss: 0.0479 - precision: 0.9628 - recall: 0.93 - ETA: 0s - loss: 0.0493 - precision: 0.9607 - recall: 0.93 - ETA: 0s - loss: 0.0493 - precision: 0.9604 - recall: 0.93\n",
      "Epoch 66/100\n",
      "23660/23660 [==============================] - ETA: 0s - loss: 0.0482 - precision: 0.9571 - recall: 0.9415 ETA: 0s - loss: 0.0480 - precision: 0.9606 - recall: 0.94 - ETA: 0s - loss: 0.0440 - precision: 0.9658 - recall: 0.95 - ETA: 0s - loss: 0.0442 - precision: 0.9663 - recall: 0.94 - ETA: 0s - loss: 0.0487 - precision: 0.9576 - recall: 0.94 - ETA: 0s - loss: 0.0476 - precision: 0.9599 - recall: 0.94 - ETA: 0s - loss: 0.0466 - precision: 0.9598 - recall: 0.94 - ETA: 0s - loss: 0.0479 - precision: 0.9575 - recall: 0. - ETA: 0s - loss: 0.0484 - precision: 0.9585 - recall: 0.94 - 1s 22us/step - loss: 0.0493 - precision: 0.9577 - recall: 0.9396\n",
      "Epoch 67/100\n",
      "23660/23660 [==============================] - 1s 25us/step - loss: 0.0498 - precision: 0.9576 - recall: 0.9395 0s - loss: 0.0421 - precision: 0.9724 - recall: 0.93 - ETA: 0s - loss: 0.0469 - precision: 0.9566 - recall: 0.93 - ETA: 0s - loss: 0.0493 - precision: 0.9472 - recall: 0.93 - ETA: 0s - loss: 0.0490 - precision: 0.9558 - recall: 0.93 - ETA: 0s - loss: 0.0485 - precision: 0.9565 - recall: 0.93 - ETA: 0s - loss: 0.0465 - precision: 0.9588 - recall: 0.94 - ETA: 0s - loss: 0.0490 - precision: 0.9584 - recall: 0.93 - ETA: 0s - loss: 0.0493 - precision: 0.9579 - recall: 0.93 - ETA: 0s - loss: 0.0484 - precision: 0.9589 - recall: 0.93 - ETA: 0s - loss: 0.0497 - precision: 0.9574 - recall: 0.93 - ETA: 0s - loss: 0.0503 - precision: 0.9566 - recall: 0.93\n",
      "Epoch 68/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23660/23660 [==============================] - 1s 26us/step - loss: 0.0479 - precision: 0.9619 - recall: 0.9371 0s - loss: 0.0510 - precision: 0.9644 - recall: 0.93 - ETA: 0s - loss: 0.0485 - precision: 0.9588 - recall: 0.94 - ETA: 0s - loss: 0.0472 - precision: 0.9624 - recall: 0.94 - ETA: 0s - loss: 0.0467 - precision: 0.9625 - recall: 0.94 - ETA: 0s - loss: 0.0467 - precision: 0.9629 - recall: 0.93 - ETA: 0s - loss: 0.0472 - precision: 0.9610 - recall: 0.93 - ETA: 0s - loss: 0.0467 - precision: 0.9618 - recall: 0.93 - ETA: 0s - loss: 0.0468 - precision: 0.9604 - recall: 0.93 - ETA: 0s - loss: 0.0482 - precision: 0.9599 - recall: 0.93 - ETA: 0s - loss: 0.0484 - precision: 0.9594 - recall: 0.93 - ETA: 0s - loss: 0.0482 - precision: 0.9600 - recall: 0.93 - ETA: 0s - loss: 0.0480 - precision: 0.9616 - recall: 0.93\n",
      "Epoch 69/100\n",
      "23660/23660 [==============================] - ETA: 0s - loss: 0.0376 - precision: 1.0000 - recall: 0.95 - ETA: 0s - loss: 0.0440 - precision: 0.9637 - recall: 0.94 - ETA: 0s - loss: 0.0458 - precision: 0.9545 - recall: 0.94 - ETA: 0s - loss: 0.0426 - precision: 0.9567 - recall: 0.94 - ETA: 0s - loss: 0.0443 - precision: 0.9588 - recall: 0.94 - ETA: 0s - loss: 0.0447 - precision: 0.9605 - recall: 0.93 - ETA: 0s - loss: 0.0451 - precision: 0.9596 - recall: 0.94 - ETA: 0s - loss: 0.0449 - precision: 0.9604 - recall: 0.94 - ETA: 0s - loss: 0.0449 - precision: 0.9614 - recall: 0.94 - ETA: 0s - loss: 0.0460 - precision: 0.9612 - recall: 0.94 - ETA: 0s - loss: 0.0457 - precision: 0.9610 - recall: 0.94 - ETA: 0s - loss: 0.0468 - precision: 0.9603 - recall: 0.93 - 1s 26us/step - loss: 0.0477 - precision: 0.9590 - recall: 0.9393\n",
      "Epoch 70/100\n",
      "23660/23660 [==============================] - ETA: 0s - loss: 0.0386 - precision: 1.0000 - recall: 0.90 - ETA: 0s - loss: 0.0511 - precision: 0.9412 - recall: 0.94 - ETA: 0s - loss: 0.0452 - precision: 0.9496 - recall: 0.94 - ETA: 0s - loss: 0.0459 - precision: 0.9568 - recall: 0.94 - ETA: 0s - loss: 0.0454 - precision: 0.9554 - recall: 0.94 - ETA: 0s - loss: 0.0461 - precision: 0.9567 - recall: 0.93 - ETA: 0s - loss: 0.0461 - precision: 0.9565 - recall: 0.94 - ETA: 0s - loss: 0.0467 - precision: 0.9591 - recall: 0.93 - ETA: 0s - loss: 0.0473 - precision: 0.9571 - recall: 0.93 - ETA: 0s - loss: 0.0464 - precision: 0.9599 - recall: 0.94 - ETA: 0s - loss: 0.0468 - precision: 0.9602 - recall: 0.94 - ETA: 0s - loss: 0.0473 - precision: 0.9601 - recall: 0.93 - 1s 25us/step - loss: 0.0476 - precision: 0.9585 - recall: 0.9402\n",
      "Epoch 71/100\n",
      "23660/23660 [==============================] - ETA: 0s - loss: 0.0664 - precision: 0.9500 - recall: 0.90 - ETA: 0s - loss: 0.0409 - precision: 0.9722 - recall: 0.93 - ETA: 0s - loss: 0.0403 - precision: 0.9718 - recall: 0.94 - ETA: 0s - loss: 0.0442 - precision: 0.9647 - recall: 0.93 - ETA: 0s - loss: 0.0435 - precision: 0.9654 - recall: 0.93 - ETA: 0s - loss: 0.0441 - precision: 0.9597 - recall: 0.93 - ETA: 0s - loss: 0.0446 - precision: 0.9594 - recall: 0.93 - ETA: 0s - loss: 0.0442 - precision: 0.9606 - recall: 0.94 - ETA: 0s - loss: 0.0445 - precision: 0.9621 - recall: 0.94 - ETA: 0s - loss: 0.0462 - precision: 0.9592 - recall: 0.94 - ETA: 0s - loss: 0.0459 - precision: 0.9587 - recall: 0.94 - ETA: 0s - loss: 0.0475 - precision: 0.9575 - recall: 0.93 - 1s 25us/step - loss: 0.0472 - precision: 0.9571 - recall: 0.9406\n",
      "Epoch 72/100\n",
      "23660/23660 [==============================] - ETA: 0s - loss: 0.0460 - precision: 0.9588 - recall: 0.9432 ETA: 0s - loss: 0.0497 - precision: 0.9527 - recall: 0.95 - ETA: 0s - loss: 0.0481 - precision: 0.9584 - recall: 0.94 - ETA: 0s - loss: 0.0462 - precision: 0.9603 - recall: 0.94 - ETA: 0s - loss: 0.0452 - precision: 0.9585 - recall: 0.94 - ETA: 0s - loss: 0.0448 - precision: 0.9601 - recall: 0. - ETA: 0s - loss: 0.0467 - precision: 0.9575 - recall: 0.94 - ETA: 0s - loss: 0.0470 - precision: 0.9588 - recall: 0.94 - ETA: 0s - loss: 0.0469 - precision: 0.9588 - recall: 0.94 - ETA: 0s - loss: 0.0472 - precision: 0.9598 - recall: 0.94 - ETA: 0s - loss: 0.0484 - precision: 0.9568 - recall: 0.94 - 1s 26us/step - loss: 0.0480 - precision: 0.9581 - recall: 0.9408\n",
      "Epoch 73/100\n",
      "23660/23660 [==============================] - ETA: 0s - loss: 0.0402 - precision: 0.9689 - recall: 0.9518 ETA: 0s - loss: 0.0351 - precision: 0.9824 - recall: 0.96 - ETA: 0s - loss: 0.0403 - precision: 0.9740 - recall: 0. - ETA: 0s - loss: 0.0397 - precision: 0.9706 - recall: 0. - ETA: 0s - loss: 0.0418 - precision: 0.9651 - recall: 0.94 - ETA: 0s - loss: 0.0420 - precision: 0.9647 - recall: 0.94 - ETA: 0s - loss: 0.0437 - precision: 0.9623 - recall: 0.94 - ETA: 0s - loss: 0.0444 - precision: 0.9626 - recall: 0.94 - ETA: 0s - loss: 0.0457 - precision: 0.9610 - recall: 0.94 - 1s 24us/step - loss: 0.0467 - precision: 0.9608 - recall: 0.9435\n",
      "Epoch 74/100\n",
      "23660/23660 [==============================] - ETA: 0s - loss: 0.0430 - precision: 0.9631 - recall: 0.9475 ETA: 0s - loss: 0.0390 - precision: 0.9732 - recall: 0.94 - ETA: 0s - loss: 0.0401 - precision: 0.9662 - recall: 0.94 - ETA: 0s - loss: 0.0419 - precision: 0.9643 - recall:  - ETA: 0s - loss: 0.0441 - precision: 0.9606 - recall: 0.94 - ETA: 0s - loss: 0.0437 - precision: 0.9608 - recall: 0.94 - ETA: 0s - loss: 0.0449 - precision: 0.9599 - recall: 0.94 - ETA: 0s - loss: 0.0453 - precision: 0.9609 - recall: 0.94 - ETA: 0s - loss: 0.0459 - precision: 0.9593 - recall: 0.94 - 1s 22us/step - loss: 0.0458 - precision: 0.9593 - recall: 0.9441\n",
      "Epoch 75/100\n",
      "23660/23660 [==============================] - ETA: 0s - loss: 0.0460 - precision: 0.9651 - recall: 0.9436 ETA: 0s - loss: 0.0609 - precision: 0.9562 - recall: 0. - ETA: 0s - loss: 0.0445 - precision: 0.9634 - recall: 0.94 - ETA: 0s - loss: 0.0458 - precision: 0.9609 - recall: 0.94 - ETA: 0s - loss: 0.0460 - precision: 0.9602 - recall: 0.94 - ETA: 0s - loss: 0.0447 - precision: 0.9641 - recall: 0.94 - ETA: 0s - loss: 0.0452 - precision: 0.9628 - recall: 0.94 - ETA: 0s - loss: 0.0457 - precision: 0.9629 - recall: 0.94 - ETA: 0s - loss: 0.0449 - precision: 0.9629 - recall: 0.94 - ETA: 0s - loss: 0.0459 - precision: 0.9617 - recall: 0.94 - ETA: 0s - loss: 0.0454 - precision: 0.9614 - recall: 0.94 - ETA: 0s - loss: 0.0463 - precision: 0.9590 - recall: 0.94 - 1s 26us/step - loss: 0.0462 - precision: 0.9591 - recall: 0.9428\n",
      "Epoch 76/100\n",
      "23660/23660 [==============================] - ETA: 0s - loss: 0.0412 - precision: 0.9688 - recall: 0.9467 ETA: 0s - loss: 0.0440 - precision: 0.9647 - recall: 0.93 - ETA: 0s - loss: 0.0432 - precision: 0.9711 - recall - ETA: 0s - loss: 0.0426 - precision: 0.9652 - recall: 0.94 - ETA: 0s - loss: 0.0431 - precision: 0.9629 - recall: 0.94 - ETA: 0s - loss: 0.0450 - precision: 0.9610 - recall: 0.94 - ETA: 0s - loss: 0.0443 - precision: 0.9618 - recall: 0.94 - ETA: 0s - loss: 0.0448 - precision: 0.9613 - recall: 0.94 - ETA: 0s - loss: 0.0446 - precision: 0.9623 - recall: 0.94 - ETA: 0s - loss: 0.0443 - precision: 0.9620 - recall: 0.94 - 1s 26us/step - loss: 0.0447 - precision: 0.9604 - recall: 0.9426\n",
      "Epoch 77/100\n",
      "23660/23660 [==============================] - ETA: 0s - loss: 0.0742 - precision: 1.0000 - recall: 0.94 - ETA: 0s - loss: 0.0550 - precision: 0.9605 - recall: 0.93 - ETA: 0s - loss: 0.0473 - precision: 0.9626 - recall: 0.94 - ETA: 0s - loss: 0.0466 - precision: 0.9645 - recall: 0.93 - ETA: 0s - loss: 0.0467 - precision: 0.9601 - recall: 0.93 - ETA: 0s - loss: 0.0472 - precision: 0.9603 - recall: 0.93 - ETA: 0s - loss: 0.0462 - precision: 0.9598 - recall: 0.94 - ETA: 0s - loss: 0.0482 - precision: 0.9590 - recall: 0.94 - ETA: 0s - loss: 0.0474 - precision: 0.9607 - recall: 0.94 - ETA: 0s - loss: 0.0464 - precision: 0.9609 - recall: 0.94 - ETA: 0s - loss: 0.0460 - precision: 0.9613 - recall: 0.94 - ETA: 0s - loss: 0.0460 - precision: 0.9599 - recall: 0.94 - 1s 25us/step - loss: 0.0461 - precision: 0.9597 - recall: 0.9428\n",
      "Epoch 78/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23660/23660 [==============================] - ETA: 0s - loss: 0.0417 - precision: 0.9681 - recall: 0.9494 ETA: 0s - loss: 0.0347 - precision: 0.9670 - recall: 0.95 - ETA: 0s - loss: 0.0399 - precision: 0.9724 - recall: 0.94 - ETA: 0s - loss: 0.0410 - precision: 0.9656 - recall: 0.95 - ETA: 0s - loss: 0.0429 - precision: 0.9649 - recall: 0. - ETA: 0s - loss: 0.0434 - precision: 0.9640 - recall: 0.94 - ETA: 0s - loss: 0.0432 - precision: 0.9659 - recall: 0.94 - ETA: 0s - loss: 0.0431 - precision: 0.9651 - recall: 0.94 - ETA: 0s - loss: 0.0425 - precision: 0.9652 - recall: 0.94 - ETA: 0s - loss: 0.0435 - precision: 0.9655 - recall: 0.94 - ETA: 0s - loss: 0.0433 - precision: 0.9668 - recall: 0.94 - ETA: 0s - loss: 0.0435 - precision: 0.9657 - recall: 0.94 - 1s 27us/step - loss: 0.0439 - precision: 0.9660 - recall: 0.9444\n",
      "Epoch 79/100\n",
      "23660/23660 [==============================] - 1s 27us/step - loss: 0.0436 - precision: 0.9621 - recall: 0.9477 0s - loss: 0.0346 - precision: 0.9752 - recall: 0.96 - ETA: 0s - loss: 0.0388 - precision: 0.9723 - recall: 0.96 - ETA: 0s - loss: 0.0424 - precision: 0.9642 - recall: 0.95 - ETA: 0s - loss: 0.0419 - precision: 0.9648 - recall: 0.95 - ETA: 0s - loss: 0.0419 - precision: 0.9642 - recall: 0.95 - ETA: 0s - loss: 0.0409 - precision: 0.9650 - recall: 0.95 - ETA: 0s - loss: 0.0403 - precision: 0.9652 - recall: 0.95 - ETA: 0s - loss: 0.0409 - precision: 0.9656 - recall: 0.95 - ETA: 0s - loss: 0.0428 - precision: 0.9625 - recall: 0.95 - ETA: 0s - loss: 0.0432 - precision: 0.9638 - recall: 0.94 - ETA: 0s - loss: 0.0432 - precision: 0.9634 - recall: 0.94 - ETA: 0s - loss: 0.0436 - precision: 0.9624 - recall: 0.94\n",
      "Epoch 80/100\n",
      "23660/23660 [==============================] - ETA: 0s - loss: 0.0449 - precision: 0.9651 - recall: 0.9428 ETA: 0s - loss: 0.0456 - precision: 0.9721 - recall: 0.94 - ETA: 0s - loss: 0.0454 - precision: 0.9723 - recall: 0.93 - ETA: 0s - loss: 0.0434 - precision: 0.9702 - recall:  - ETA: 0s - loss: 0.0460 - precision: 0.9628 - recall: 0.94 - ETA: 0s - loss: 0.0451 - precision: 0.9633 - recall: 0.94 - ETA: 0s - loss: 0.0447 - precision: 0.9635 - recall: 0.94 - ETA: 0s - loss: 0.0446 - precision: 0.9637 - recall: 0.94 - ETA: 0s - loss: 0.0449 - precision: 0.9628 - recall: 0.94 - 1s 23us/step - loss: 0.0446 - precision: 0.9632 - recall: 0.9450\n",
      "Epoch 81/100\n",
      "23660/23660 [==============================] - ETA: 0s - loss: 0.0429 - precision: 0.9641 - recall: 0.9485 ETA: 0s - loss: 0.0350 - precision: 0.9719 - recall: 0.97 - ETA: 0s - loss: 0.0386 - precision: 0.9664 - recall: 0.96 - ETA: 0s - loss: 0.0401 - precision: 0.9703 - recall: 0.95 - ETA: 0s - loss: 0.0425 - precision: 0.9652 - recall: 0.95 - ETA: 0s - loss: 0.0430 - precision: 0.9643 - recall: 0.95 - ETA: 0s - loss: 0.0425 - precision: 0.9655 - recall: 0.95 - ETA: 0s - loss: 0.0423 - precision: 0.9655 - recall:  - ETA: 0s - loss: 0.0424 - precision: 0.9645 - recall: 0.94 - ETA: 0s - loss: 0.0425 - precision: 0.9636 - recall: 0.94 - 1s 24us/step - loss: 0.0424 - precision: 0.9638 - recall: 0.9481\n",
      "Epoch 82/100\n",
      "23660/23660 [==============================] - ETA: 0s - loss: 0.0154 - precision: 1.0000 - recall: 1.00 - ETA: 0s - loss: 0.0372 - precision: 0.9633 - recall: 0.96 - ETA: 0s - loss: 0.0392 - precision: 0.9636 - recall: 0.96 - ETA: 0s - loss: 0.0387 - precision: 0.9670 - recall: 0.95 - ETA: 0s - loss: 0.0386 - precision: 0.9696 - recall: 0.95 - ETA: 0s - loss: 0.0397 - precision: 0.9660 - recall: 0.95 - ETA: 0s - loss: 0.0394 - precision: 0.9693 - recall: 0.95 - ETA: 0s - loss: 0.0392 - precision: 0.9696 - recall: 0.95 - ETA: 0s - loss: 0.0394 - precision: 0.9690 - recall: 0.95 - ETA: 0s - loss: 0.0401 - precision: 0.9690 - recall: 0.95 - ETA: 0s - loss: 0.0401 - precision: 0.9681 - recall: 0.95 - ETA: 0s - loss: 0.0404 - precision: 0.9669 - recall: 0.95 - ETA: 0s - loss: 0.0409 - precision: 0.9669 - recall: 0.95 - ETA: 0s - loss: 0.0415 - precision: 0.9658 - recall: 0.95 - ETA: 0s - loss: 0.0417 - precision: 0.9655 - recall: 0.95 - 1s 31us/step - loss: 0.0420 - precision: 0.9658 - recall: 0.9496\n",
      "Epoch 83/100\n",
      "23660/23660 [==============================] - ETA: 1s - loss: 0.0190 - precision: 1.0000 - recall: 1.00 - ETA: 0s - loss: 0.0323 - precision: 0.9829 - recall: 0.96 - ETA: 0s - loss: 0.0351 - precision: 0.9805 - recall: 0.95 - ETA: 0s - loss: 0.0354 - precision: 0.9741 - recall: 0.95 - ETA: 0s - loss: 0.0368 - precision: 0.9725 - recall: 0.95 - ETA: 0s - loss: 0.0378 - precision: 0.9706 - recall: 0.95 - ETA: 0s - loss: 0.0387 - precision: 0.9688 - recall: 0.95 - ETA: 0s - loss: 0.0391 - precision: 0.9688 - recall: 0.94 - ETA: 0s - loss: 0.0406 - precision: 0.9685 - recall: 0.94 - ETA: 0s - loss: 0.0412 - precision: 0.9656 - recall: 0.94 - ETA: 0s - loss: 0.0432 - precision: 0.9621 - recall: 0.94 - ETA: 0s - loss: 0.0432 - precision: 0.9644 - recall: 0.94 - ETA: 0s - loss: 0.0430 - precision: 0.9651 - recall: 0.94 - ETA: 0s - loss: 0.0430 - precision: 0.9647 - recall: 0.94 - ETA: 0s - loss: 0.0435 - precision: 0.9632 - recall: 0.94 - 1s 31us/step - loss: 0.0434 - precision: 0.9639 - recall: 0.9463\n",
      "Epoch 84/100\n",
      "23660/23660 [==============================] - ETA: 0s - loss: 0.0417 - precision: 0.9689 - recall: 0.9497 ETA: 0s - loss: 0.0378 - precision: 0.9880 - recall: 0.95 - ETA: 0s - loss: 0.0396 - precision: 0.9810 - recall: 0. - ETA: 0s - loss: 0.0417 - precision: 0.9717 - recall: 0.95 - ETA: 0s - loss: 0.0425 - precision: 0.9725 - recall: 0.94 - ETA: 0s - loss: 0.0422 - precision: 0.9690 - recall: 0. - ETA: 0s - loss: 0.0413 - precision: 0.9683 - recall: 0.94 - ETA: 0s - loss: 0.0409 - precision: 0.9683 - recall: 0.94 - ETA: 0s - loss: 0.0409 - precision: 0.9689 - recall: 0.94 - ETA: 0s - loss: 0.0416 - precision: 0.9672 - recall: 0.94 - ETA: 0s - loss: 0.0414 - precision: 0.9670 - recall: 0.94 - ETA: 0s - loss: 0.0420 - precision: 0.9664 - recall: 0.94 - ETA: 0s - loss: 0.0416 - precision: 0.9666 - recall: 0.94 - ETA: 0s - loss: 0.0415 - precision: 0.9661 - recall: 0.94 - 1s 35us/step - loss: 0.0421 - precision: 0.9650 - recall: 0.9468\n",
      "Epoch 85/100\n",
      "23660/23660 [==============================] - 1s 36us/step - loss: 0.0428 - precision: 0.9645 - recall: 0.9469 0s - loss: 0.0374 - precision: 0.9721 - recall: 0.96 - ETA: 0s - loss: 0.0410 - precision: 0.9687 - recall: 0.95 - ETA: 0s - loss: 0.0406 - precision: 0.9707 - recall: 0.94 - ETA: 0s - loss: 0.0397 - precision: 0.9690 - recall: 0.95 - ETA: 0s - loss: 0.0393 - precision: 0.9710 - recall: 0.94 - ETA: 0s - loss: 0.0381 - precision: 0.9690 - recall: 0.95 - ETA: 0s - loss: 0.0378 - precision: 0.9696 - recall: 0.95 - ETA: 0s - loss: 0.0386 - precision: 0.9686 - recall: 0.95 - ETA: 0s - loss: 0.0382 - precision: 0.9708 - recall: 0. - ETA: 0s - loss: 0.0388 - precision: 0.9682 - recall: 0.95 - ETA: 0s - loss: 0.0392 - precision: 0.9682 - recall: 0.95 - ETA: 0s - loss: 0.0391 - precision: 0.9676 - recall: 0.95 - ETA: 0s - loss: 0.0404 - precision: 0.9676 - recall: 0.94 - ETA: 0s - loss: 0.0415 - precision: 0.9657 - recall: 0.\n",
      "Epoch 86/100\n",
      "23660/23660 [==============================] - ETA: 0s - loss: 0.0435 - precision: 0.9658 - recall: 0.94 - ETA: 0s - loss: 0.0430 - precision: 0.9655 - recall: 0.95 - ETA: 0s - loss: 0.0428 - precision: 0.9667 - recall: 0.95 - ETA: 0s - loss: 0.0438 - precision: 0.9652 - recall: 0.94 - ETA: 0s - loss: 0.0438 - precision: 0.9640 - recall: 0.94 - ETA: 0s - loss: 0.0433 - precision: 0.9646 - recall: 0.94 - ETA: 0s - loss: 0.0428 - precision: 0.9632 - recall: 0.95 - ETA: 0s - loss: 0.0430 - precision: 0.9638 - recall: 0.94 - 1s 25us/step - loss: 0.0426 - precision: 0.9646 - recall: 0.9495\n",
      "Epoch 87/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23660/23660 [==============================] - 1s 28us/step - loss: 0.0413 - precision: 0.9652 - recall: 0.9510 0s - loss: 0.0386 - precision: 0.9504 - recall: 0.93 - ETA: 0s - loss: 0.0355 - precision: 0.9676 - recall: 0.94 - ETA: 0s - loss: 0.0338 - precision: 0.9689 - recall: 0.95 - ETA: 0s - loss: 0.0342 - precision: 0.9690 - recall: 0.95 - ETA: 0s - loss: 0.0362 - precision: 0.9697 - recall: 0.95 - ETA: 0s - loss: 0.0374 - precision: 0.9699 - recall: 0.95 - ETA: 0s - loss: 0.0391 - precision: 0.9677 - recall: 0.95 - ETA: 0s - loss: 0.0403 - precision: 0.9686 - recall: 0. - ETA: 0s - loss: 0.0406 - precision: 0.9676 - recall: 0.95 - ETA: 0s - loss: 0.0400 - precision: 0.9678 - recall: 0.95 - ETA: 0s - loss: 0.0407 - precision: 0.9658 - recall: 0.95\n",
      "Epoch 88/100\n",
      "23660/23660 [==============================] - ETA: 0s - loss: 0.0394 - precision: 0.9734 - recall: 0.9467 ETA: 0s - loss: 0.0311 - precision: 0.9880 - recall:  - ETA: 0s - loss: 0.0397 - precision: 0.9727 - recall: 0.94 - ETA: 0s - loss: 0.0390 - precision: 0.9726 - recall: 0.95 - ETA: 0s - loss: 0.0403 - precision: 0.9699 - recall: 0.94 - ETA: 0s - loss: 0.0396 - precision: 0.9723 - recall: 0.94 - ETA: 0s - loss: 0.0395 - precision: 0.9715 - recall: 0.95 - ETA: 0s - loss: 0.0403 - precision: 0.9707 - recall: 0.94 - ETA: 0s - loss: 0.0400 - precision: 0.9709 - recall: 0.95 - ETA: 0s - loss: 0.0406 - precision: 0.9686 - recall: 0.95 - ETA: 0s - loss: 0.0405 - precision: 0.9696 - recall: 0.94 - 1s 27us/step - loss: 0.0404 - precision: 0.9692 - recall: 0.9492\n",
      "Epoch 89/100\n",
      "23660/23660 [==============================] - ETA: 0s - loss: 0.0101 - precision: 1.0000 - recall: 1.00 - ETA: 0s - loss: 0.0300 - precision: 0.9792 - recall: 0.96 - ETA: 0s - loss: 0.0318 - precision: 0.9771 - recall: 0.96 - ETA: 0s - loss: 0.0317 - precision: 0.9761 - recall: 0.96 - ETA: 0s - loss: 0.0336 - precision: 0.9755 - recall: 0.96 - ETA: 0s - loss: 0.0319 - precision: 0.9745 - recall: 0.96 - ETA: 0s - loss: 0.0335 - precision: 0.9724 - recall: 0.95 - ETA: 0s - loss: 0.0344 - precision: 0.9734 - recall: 0.95 - ETA: 0s - loss: 0.0345 - precision: 0.9730 - recall: 0.95 - ETA: 0s - loss: 0.0357 - precision: 0.9700 - recall: 0.95 - ETA: 0s - loss: 0.0375 - precision: 0.9700 - recall: 0.95 - ETA: 0s - loss: 0.0380 - precision: 0.9674 - recall: 0.95 - ETA: 0s - loss: 0.0385 - precision: 0.9680 - recall: 0.95 - ETA: 0s - loss: 0.0401 - precision: 0.9645 - recall: 0.94 - 1s 29us/step - loss: 0.0400 - precision: 0.9643 - recall: 0.9501\n",
      "Epoch 90/100\n",
      "23660/23660 [==============================] - ETA: 0s - loss: 0.0330 - precision: 1.0000 - recall: 1.00 - ETA: 0s - loss: 0.0352 - precision: 0.9626 - recall: 0.96 - ETA: 0s - loss: 0.0378 - precision: 0.9690 - recall: 0.95 - ETA: 0s - loss: 0.0384 - precision: 0.9687 - recall: 0.95 - ETA: 0s - loss: 0.0404 - precision: 0.9657 - recall: 0.94 - ETA: 0s - loss: 0.0392 - precision: 0.9697 - recall: 0.95 - ETA: 0s - loss: 0.0409 - precision: 0.9652 - recall: 0.94 - ETA: 0s - loss: 0.0421 - precision: 0.9629 - recall: 0.94 - ETA: 0s - loss: 0.0420 - precision: 0.9625 - recall: 0.94 - ETA: 0s - loss: 0.0414 - precision: 0.9624 - recall: 0.94 - ETA: 0s - loss: 0.0406 - precision: 0.9636 - recall: 0.95 - ETA: 0s - loss: 0.0405 - precision: 0.9643 - recall: 0.95 - ETA: 0s - loss: 0.0401 - precision: 0.9651 - recall: 0.95 - ETA: 0s - loss: 0.0405 - precision: 0.9654 - recall: 0.94 - 1s 29us/step - loss: 0.0405 - precision: 0.9653 - recall: 0.9494\n",
      "Epoch 91/100\n",
      "23660/23660 [==============================] - 1s 27us/step - loss: 0.0380 - precision: 0.9695 - recall: 0.9550 0s - loss: 0.0432 - precision: 0.9664 - recall: 0.95 - ETA: 0s - loss: 0.0417 - precision: 0.9636 - recall: 0.95 - ETA: 0s - loss: 0.0400 - precision: 0.9663 - recall: 0.95 - ETA: 0s - loss: 0.0389 - precision: 0.9695 - recall: 0.95 - ETA: 0s - loss: 0.0383 - precision: 0.9685 - recall: 0.96 - ETA: 0s - loss: 0.0382 - precision: 0.9692 - recall: 0.95 - ETA: 0s - loss: 0.0382 - precision: 0.9686 - recall: 0.95 - ETA: 0s - loss: 0.0389 - precision: 0.9697 - recall: 0.95 - ETA: 0s - loss: 0.0381 - precision: 0.9704 - recall: 0.95 - ETA: 0s - loss: 0.0378 - precision: 0.9708 - recall: 0.95 - ETA: 0s - loss: 0.0378 - precision: 0.9700 - recall: 0.95 - ETA: 0s - loss: 0.0379 - precision: 0.9701 - recall: 0.95\n",
      "Epoch 92/100\n",
      "23660/23660 [==============================] - ETA: 0s - loss: 0.0388 - precision: 0.9683 - recall: 0.9485 ETA: 0s - loss: 0.0381 - precision: 0.9752 - recall: 0.95 - ETA: 0s - loss: 0.0417 - precision: 0.9674 - recall: 0.94 - ETA: 0s - loss: 0.0396 - precision: 0.9695 - recall: 0.94 - ETA: 0s - loss: 0.0390 - precision: 0.9698 - recall: 0.94 - ETA: 0s - loss: 0.0389 - precision: 0.9689 - recall: 0.94 - ETA: 0s - loss: 0.0391 - precision: 0.9698 - recall: 0.94 - ETA: 0s - loss: 0.0385 - precision: 0.9709 - recall: 0.94 - ETA: 0s - loss: 0.0384 - precision: 0.9696 - recall: 0. - 1s 22us/step - loss: 0.0392 - precision: 0.9676 - recall: 0.9489\n",
      "Epoch 93/100\n",
      "23660/23660 [==============================] - 1s 23us/step - loss: 0.0394 - precision: 0.9665 - recall: 0.9518 0s - loss: 0.0355 - precision: 0.9739 - recall: 0.96 - ETA: 0s - loss: 0.0359 - precision: 0.9671 - recall: 0.96 - ETA: 0s - loss: 0.0364 - precision: 0.9660 - recall: 0.95 - ETA: 0s - loss: 0.0368 - precision: 0.9683 - recall: 0.95 - ETA: 0s - loss: 0.0363 - precision: 0.9683 - recall: 0.95 - ETA: 0s - loss: 0.0361 - precision: 0.9688 - recall: 0.95 - ETA: 0s - loss: 0.0363 - precision: 0.9691 - recall: 0.95 - ETA: 0s - loss: 0.0372 - precision: 0.9683 - recall: 0.95 - ETA: 0s - loss: 0.0380 - precision: 0.9676 - recall: 0.95 - ETA: 0s - loss: 0.0392 - precision: 0.9663 - recall: 0.95\n",
      "Epoch 94/100\n",
      "23660/23660 [==============================] - ETA: 0s - loss: 0.0234 - precision: 1.0000 - recall: 0.94 - ETA: 0s - loss: 0.0349 - precision: 0.9753 - recall: 0.95 - ETA: 0s - loss: 0.0393 - precision: 0.9716 - recall: 0.94 - ETA: 0s - loss: 0.0378 - precision: 0.9701 - recall: 0.95 - ETA: 0s - loss: 0.0384 - precision: 0.9698 - recall: 0.95 - ETA: 0s - loss: 0.0383 - precision: 0.9690 - recall: 0.95 - ETA: 0s - loss: 0.0368 - precision: 0.9697 - recall: 0.95 - ETA: 0s - loss: 0.0391 - precision: 0.9682 - recall: 0.95 - ETA: 0s - loss: 0.0409 - precision: 0.9676 - recall: 0.95 - ETA: 0s - loss: 0.0412 - precision: 0.9672 - recall: 0.95 - ETA: 0s - loss: 0.0408 - precision: 0.9679 - recall: 0.95 - ETA: 0s - loss: 0.0405 - precision: 0.9681 - recall: 0.95 - ETA: 0s - loss: 0.0400 - precision: 0.9680 - recall: 0.95 - 1s 26us/step - loss: 0.0399 - precision: 0.9681 - recall: 0.9529\n",
      "Epoch 95/100\n",
      "23660/23660 [==============================] - ETA: 0s - loss: 0.0382 - precision: 0.9687 - recall: 0.9579 ETA: 0s - loss: 0.0357 - precision: 0.9739 - recall: 0.95 - ETA: 0s - loss: 0.0329 - precision: 0.9705 - recall: 0.96 - ETA: 0s - loss: 0.0356 - precision: 0.9716 - recall: 0.95 - ETA: 0s - loss: 0.0371 - precision: 0.9674 - recall: 0.95 - ETA: 0s - loss: 0.0378 - precision: 0.9697 - recall: 0. - ETA: 0s - loss: 0.0380 - precision: 0.9703 - recall: 0.95 - ETA: 0s - loss: 0.0379 - precision: 0.9690 - recall: 0.95 - ETA: 0s - loss: 0.0383 - precision: 0.9684 - recall: 0.95 - ETA: 0s - loss: 0.0385 - precision: 0.9692 - recall: 0.95 - 1s 23us/step - loss: 0.0383 - precision: 0.9696 - recall: 0.9549\n",
      "Epoch 96/100\n",
      "23660/23660 [==============================] - ETA: 0s - loss: 0.0249 - precision: 0.9375 - recall: 1.00 - ETA: 0s - loss: 0.0376 - precision: 0.9727 - recall: 0.95 - ETA: 0s - loss: 0.0370 - precision: 0.9740 - recall: 0.95 - ETA: 0s - loss: 0.0355 - precision: 0.9750 - recall: 0.95 - ETA: 0s - loss: 0.0375 - precision: 0.9708 - recall: 0.95 - ETA: 0s - loss: 0.0377 - precision: 0.9697 - recall: 0.95 - ETA: 0s - loss: 0.0377 - precision: 0.9695 - recall: 0.95 - ETA: 0s - loss: 0.0380 - precision: 0.9684 - recall: 0.95 - ETA: 0s - loss: 0.0389 - precision: 0.9666 - recall: 0.95 - ETA: 0s - loss: 0.0389 - precision: 0.9667 - recall: 0.95 - ETA: 0s - loss: 0.0382 - precision: 0.9684 - recall: 0.95 - ETA: 0s - loss: 0.0375 - precision: 0.9694 - recall: 0.95 - 1s 25us/step - loss: 0.0372 - precision: 0.9693 - recall: 0.9523\n",
      "Epoch 97/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23660/23660 [==============================] - ETA: 0s - loss: 0.0324 - precision: 0.9757 - recall: 0.9627 ETA: 0s - loss: 0.0372 - precision: 0.9645 - recall: 0.94 - ETA: 0s - loss: 0.0334 - precision: 0.9721 - recall: 0.95 - ETA: 0s - loss: 0.0333 - precision: 0.9739 - recall: 0. - ETA: 0s - loss: 0.0324 - precision: 0.9717 - recall: 0.96 - ETA: 0s - loss: 0.0338 - precision: 0.9717 - recall: 0.95 - ETA: 0s - loss: 0.0349 - precision: 0.9673 - recall: 0.95 - ETA: 0s - loss: 0.0365 - precision: 0.9669 - recall: 0.95 - ETA: 0s - loss: 0.0367 - precision: 0.9659 - recall: 0.95 - ETA: 0s - loss: 0.0364 - precision: 0.9654 - recall: 0.95 - ETA: 0s - loss: 0.0372 - precision: 0.9651 - recall: 0.95 - 1s 24us/step - loss: 0.0373 - precision: 0.9651 - recall: 0.9540\n",
      "Epoch 98/100\n",
      "23660/23660 [==============================] - ETA: 0s - loss: 0.0406 - precision: 1.0000 - recall: 0.96 - ETA: 0s - loss: 0.0324 - precision: 0.9750 - recall: 0.96 - ETA: 0s - loss: 0.0335 - precision: 0.9751 - recall: 0.95 - ETA: 0s - loss: 0.0326 - precision: 0.9756 - recall: 0.96 - ETA: 0s - loss: 0.0327 - precision: 0.9745 - recall: 0.96 - ETA: 0s - loss: 0.0336 - precision: 0.9724 - recall: 0.95 - ETA: 0s - loss: 0.0344 - precision: 0.9700 - recall: 0.96 - ETA: 0s - loss: 0.0354 - precision: 0.9702 - recall: 0.95 - ETA: 0s - loss: 0.0354 - precision: 0.9701 - recall: 0.95 - ETA: 0s - loss: 0.0355 - precision: 0.9694 - recall: 0.95 - ETA: 0s - loss: 0.0376 - precision: 0.9684 - recall: 0.95 - 1s 24us/step - loss: 0.0371 - precision: 0.9687 - recall: 0.9573\n",
      "Epoch 99/100\n",
      "23660/23660 [==============================] - ETA: 0s - loss: 0.0105 - precision: 1.0000 - recall: 1.00 - ETA: 0s - loss: 0.0342 - precision: 0.9679 - recall: 0.95 - ETA: 0s - loss: 0.0333 - precision: 0.9771 - recall: 0.95 - ETA: 0s - loss: 0.0324 - precision: 0.9771 - recall: 0.95 - ETA: 0s - loss: 0.0341 - precision: 0.9720 - recall: 0.95 - ETA: 0s - loss: 0.0346 - precision: 0.9726 - recall: 0.95 - ETA: 0s - loss: 0.0352 - precision: 0.9723 - recall: 0.95 - ETA: 0s - loss: 0.0350 - precision: 0.9727 - recall: 0.95 - ETA: 0s - loss: 0.0349 - precision: 0.9714 - recall: 0.95 - ETA: 0s - loss: 0.0360 - precision: 0.9708 - recall: 0.95 - ETA: 0s - loss: 0.0365 - precision: 0.9699 - recall: 0.95 - ETA: 0s - loss: 0.0360 - precision: 0.9710 - recall: 0.95 - 1s 26us/step - loss: 0.0364 - precision: 0.9709 - recall: 0.9567\n",
      "Epoch 100/100\n",
      "23660/23660 [==============================] - ETA: 0s - loss: 0.0754 - precision: 0.9000 - recall: 0.94 - ETA: 0s - loss: 0.0304 - precision: 0.9685 - recall: 0.95 - ETA: 0s - loss: 0.0300 - precision: 0.9762 - recall: 0.95 - ETA: 0s - loss: 0.0321 - precision: 0.9722 - recall: 0.95 - ETA: 0s - loss: 0.0331 - precision: 0.9705 - recall: 0.95 - ETA: 0s - loss: 0.0359 - precision: 0.9696 - recall: 0.95 - ETA: 0s - loss: 0.0358 - precision: 0.9677 - recall: 0.95 - ETA: 0s - loss: 0.0358 - precision: 0.9670 - recall: 0.95 - ETA: 0s - loss: 0.0373 - precision: 0.9660 - recall: 0.95 - ETA: 0s - loss: 0.0373 - precision: 0.9670 - recall: 0.95 - 0s 21us/step - loss: 0.0370 - precision: 0.9672 - recall: 0.9534\n"
     ]
    }
   ],
   "source": [
    "inputl:inp[`shape pykw enlist 51]\n",
    "hidden1:dense[40;`activation pykw `relu][inputl]\n",
    "hidden2:dense[40;`activation pykw `relu][hidden1]\n",
    "o:dense[1;`activation pykw `sigmoid][hidden2]\n",
    "\n",
    "modl:models[`inputs pykw inputl;`outputs pykw o]\n",
    "modl[`:compile][`optimizer pykw \"adam\";`loss pykw \"binary_crossentropy\";`metrics pykw (km[`:binary_precision][]`;km[`:binary_recall][]`)];\n",
    "modl[`:summary][];\n",
    "\n",
    "res:modl[`:fit][array[xtr];ytr;`batch_size pykw 100;`verbose pykw 1;`epochs pykw 100];\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.864569e-06 2.682209e-07 3.874302e-07 5.960464e-08 7.602572e-05 0.00533393 0..\n"
     ]
    }
   ],
   "source": [
    "show nnPred:raze(modl[`:predict]array[mattab xtest])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss     | 0.2206197 0.1478676 0.1331592 0.1235214 0.1169168 0.1110304 0.1071..\n",
      "precision| 0.9510231 0.9315508 0.9272867 0.9238668 0.9289941 0.9268707 0.9220..\n",
      "recall   | 0.623488  0.7653398 0.7921707 0.8113042 0.8262591 0.8370354 0.8484..\n",
      "loss     | 0.2206197 0.1478676 0.1331592 0.1235214 0.1169168 0.1110304 0.1071..\n",
      "precision| 0.9510231 0.9315508 0.9272867 0.9238668 0.9289941 0.9268707 0.9220..\n",
      "recall   | 0.623488  0.7653398 0.7921707 0.8113042 0.8262591 0.8370354 0.8484..\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 460.8x345.6 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the model is 0.9641493\n",
      "The mean_class_accuracy of the model is 0.9746701\n"
     ]
    }
   ],
   "source": [
    "show hist:6#(res[`:history]`)\n",
    "\n",
    "plt[`:plot]hist`loss;\n",
    "plt[`:plot]hist`val_loss;\n",
    "plt[`:title]\"model loss\";\n",
    "plt[`:ylabel]\"loss\";\n",
    "plt[`:xlabel]\"epoch\";\n",
    "plt[`:show][];\n",
    "\n",
    "conf:.ml.confdict[ytest;pred1;1b]\n",
    "\n",
    "acc:(count where (0.5<nnPred)=ytest)%count[ytest]\n",
    "meanclassavg:avg (conf[`tp]%(sum conf[`tp`fn]);conf[`tn]%(sum conf[`tn`fp]))\n",
    "-1\"The accuracy of the model is \",string acc;\n",
    "-1\"The mean_class_accuracy of the model is \",string meanclassavg;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0| 16729 462 \n",
      "1| 214   1451\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show cnfM:.ml.confmat[ytest;0.5<nnPred]\n",
    ".ml.displayCM[value cnfM;`Low`High;\"Test Set Confusion Matrix\";()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Q (kdb+)",
   "language": "q",
   "name": "qpk"
  },
  "language_info": {
   "file_extension": ".q",
   "mimetype": "text/x-q",
   "name": "q",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
