{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FDL 2019 - Floods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flooding is a destructive, dangerous, and common hazard. Seventy-five percent of all Presidential disaster declarations are associated with flooding [1]. The work in this memorandum focuses specifically on river flooding. The National Oceanic and Atmospheric Administration's (NOAA's) 13 river forecast centers predict river levels at specific river locations up to several days in advance using hydrologic and hydraulic modeling [1,2]. These forecasts can help cities prepare for and respond to river flooding events in the short-term. The Office of Water Prediction also prepares seasonal water forecasts at regional spatial resolution [3]. The work presented here is complementary to these efforts and explores the efficacy of statistical -- instead of hydrological -- models for the prediction and forecasting of river flooding. \n",
    "\n",
    "The scope of this project focuses on two measures of flood susceptibility: predicting whether a specific river gage site will flood in a given month, and the time to peak river level during a flood event. These questions were addressed from two perspectives. The first perspective assumes a well-gaged river basin and seeks to forecast the presence of a flood in the forthcoming month, or the time to peak river level for an ongoing rain event. The second perspective addresses the problem of predicting flood susceptibility in poorly gaged or entirely ungaged river basin. For these experiments, no prior river level information is incorporated into the predictive model. The river level data used to train the statistical models is made possible by the extensive network of stream gages maintained by the US Geological Survey (USGS). Once the models are trained, they are used to construct maps of flood susceptibility, and the most important predictors of flood susceptibility from the statistical models are identified. \n",
    "\n",
    "The dataset was collected in 6 states over a period of 10 years between 2009.07-2019.07"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in ml library and required datasets which include:\n",
    "\n",
    "    -rainfall data from PRISM\n",
    "    -Landcover information from NLCD\n",
    "    -Basin Characteristics\n",
    "    -Threshold Information per site\n",
    "    -FLASH Flood data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\\l ../../fdl2019/ml/ml.q \n",
    "\\l ../../fdl2019/ml/init.q\n",
    "\\l ../../fdl2019/ml/fresh/notebooks/graphics.q\n",
    "\n",
    "\\l models.q\n",
    "\\l columns_to_include.q\n",
    "\\l load_data.q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning up the Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All site_no should be updated so that they have at least 8 digits. Any with less will be front loaded with 0's. Any nulls or infinity values from the dataset are also deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "del:{![x;enlist (=;y;z);0b;`symbol$()]}\n",
    "conv7_fn:{`$$[7=count x;\"0\",;]x}\n",
    "precip:update conv7_fn each string each site_no from del[precipall;`ppt;0n]\n",
    "maxht:update conv7_fn each site_no from del[max_ht_str;`height;neg[0w]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add in index column for the warnings table (will be used later for nearest neighbours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "warning:update nn:i from warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PRISM dataset had an id limit of 12 digits, which means that some of the site_no digits were excluded. To not lose datapoints when joining, the remaining digits were added back in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rms:asc ds where 12<count each ds:exec distinct string each site_no from gauges \n",
    "names:0!select i by site_no from precip where site_no in `$12#'rms\n",
    "matchnames:0!select i by lat,long from precip where i in names[`x][20]\n",
    "lst1:til[26]except 21 22\n",
    "lst2:til[25]except 20\n",
    "i1:`$rms[lst1],rms[21 22]\n",
    "i2:(names[`x]lst2),matchnames`x\n",
    "precip:{![x;enlist(in;`i;z);0b;enlist[`site_no]!enlist enlist y]}/[precip;i1;i2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seasonal Information plays an important role when predicting if an area will flood and how long it will take which is why the sin and cos of each month of the year is calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi:acos -1\n",
    "precip:update x:cos(lat)*cos(long),y:cos(lat)*sin(long),z:sin(lat) from precip\n",
    "precip:`date xasc update month:`mm$date from precip\n",
    "precip:update cos_t:cos 2*pi*(month-1)%11,sin_t:sin 2*pi*(month-1)%11 from precip\n",
    "stryear:{$[x<2011;6;x<2016;11;16]}each `year$precip[`date]\n",
    "precip:update year:stryear from precip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inputs to the features are:\n",
    "\n",
    "prev_col:\n",
    "       \n",
    "       -x  = table that's being updated\n",
    "       -y  = how many previous values to calculate\n",
    "       -z  = column to apply the function to \n",
    "       -gr = what columns to group by\n",
    "       -col= new column name\n",
    "       \n",
    "window_feat:\n",
    "\n",
    "       -x  = table that's being updated\n",
    "       -y  = how many values to calculate\n",
    "       -z  = column to apply the function to \n",
    "       -wh = where to apply the funtion to \n",
    "       -gr = what columns to group by\n",
    "       -col= new column name\n",
    "       -w  = ? for select statement, ! for update "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_col:   {[x;y;z;gr;col]![x;();gr;$[1<count z;raze;]{raze{(enlist`$string[y],\"_\",string[x],\"_\",string[z])\n",
    "             !enlist (xprev;z;x)}[z;y;]each raze x}[y;col]each z]}\n",
    "window_feat:{[x;y;z;wh;gr;col;w]w[x;wh;gr;$[1<count z;raze;]{raze{(enlist `$string[y],\"_\",string[x],\"_\",string[z])\n",
    "             !enlist (max;(mavg;z;x))}[z;y;]each raze x}[y;col]each z]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing the upstream values is also an important feature to include into the dataset. The site_numbers are grouped by the first two digits with the remaining digits in ascending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "catch_site:((';#);2;($:;`site_no))\n",
    "upstr_ppt:prev_col[precip;1;`ppt;`site_no`date!(catch_site;`date);`upstr]\n",
    "upstr_height:prev_col[maxht;1;`height;`site_no`date!(catch_site;`date);`upstr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the previous x days of rainfall and stream height for each row in the respective datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_rain:prev_col[upstr_ppt;enlist 1_til 10;`ppt`upstr_ppt_1;(enlist `site_no)!enlist `site_no;`prev]\n",
    "all_height:prev_col[upstr_height;enlist 1+til 10;`height`upstr_height_1;(enlist `site_no)!enlist `site_no;`prev]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining the characteristics tables "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the basin characteristics to the NLCD landcover dataset based on site_no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_char:nlcd ij`site_no xkey basin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monthly Target Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gage sites are joined to a flood warning level using nearest neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "wlatl:raze each warning[`Latitude`Longitude],'gauges[`dec_lat_va`dec_long_v]\n",
    "tabw:.ml.clust.kd.buildtree[wlatl;2]\n",
    "gauge_val:count[warning]+til count gauges\n",
    "nnwarn:.ml.clust.kd.i.nns[;tabw;(count[warning]#0),count[gauges]#1;flip wlatl;`edist]each gauge_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "joins:([site_no:gauges`site_no]nn:nnwarn[;0];ndw:nnwarn[;1])\n",
    "floodlvl:(maxht ij joins)lj`nn xkey warning\n",
    "floodlvl[`Action`Moderate`Flood`Major]:\"F\"$'floodlvl[`Action`Moderate`Flood`Major]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "site_no  date    Action Flood Moderate Major no_Action no_Flood no_Mod no_Major\n",
      "-------------------------------------------------------------------------------\n",
      "01200000 2009.07 6      9     10       12    0         0        0      0       \n",
      "01200000 2009.08 6      9     10       12    0         0        0      0       \n",
      "01200000 2009.09 6      9     10       12    0         0        0      0       \n",
      "01200000 2009.10 6      9     10       12    0         0        0      0       \n",
      "01200000 2009.11 6      9     10       12    0         0        0      0       \n",
      "01200000 2009.12 6      9     10       12    0         0        0      0       \n",
      "01200000 2010.01 6      9     10       12    0         0        0      0       \n",
      "01200000 2010.02 6      9     10       12    0         0        0      0       \n",
      "01200000 2010.03 6      9     10       12    1         0        0      0       \n",
      "01200000 2010.04 6      9     10       12    0         0        0      0       \n",
      "01200000 2010.05 6      9     10       12    0         0        0      0       \n",
      "01200000 2010.06 6      9     10       12    0         0        0      0       \n",
      "01200000 2010.07 6      9     10       12    0         0        0      0       \n",
      "01200000 2010.08 6      9     10       12    0         0        0      0       \n",
      "01200000 2010.09 6      9     10       12    0         0        0      0       \n",
      "01200000 2010.10 6      9     10       12    0         0        0      0       \n",
      "01200000 2010.11 6      9     10       12    0         0        0      0       \n",
      "01200000 2010.12 6      9     10       12    0         0        0      0       \n",
      "01200000 2011.01 6      9     10       12    0         0        0      0       \n",
      "01200000 2011.02 6      9     10       12    0         0        0      0       \n",
      "..\n"
     ]
    }
   ],
   "source": [
    "show threshold:0!select first Action,first Flood,first Moderate,first Major,no_Action:count where height>Action\n",
    "              ,no_Flood:count where height>Flood,no_Mod:count where height>Moderate,no_Major:count where \n",
    "               height>Major by site_no,\"m\"$date from floodlvl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of this project, we only want to predict if the \"Flood\" level was reached or not for a site during a given month. Our target data is a binary label of whether this was reached or not.\n",
    "\n",
    "Any site that floods more than 28 days per month is omitted from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold:select from threshold where no_Flood<28\n",
    "threshold[`target]:threshold[`no_Flood]>0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Peak Target Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a major rain event, it is important to know how long to will take for the stream to reach peak height.\n",
    "\n",
    "The FLASH dataset will be used as ground thruth, which highlights how long it will take to reach this peak height, only dates and site numbers used in the rain and height will be extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak[`delta_peak]:(peak[`peak_time]-peak[`start_time])*24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To predict how long it will take to flood, we have classified this problem as a binary class of whether or not it will take longer than 3 hours. This is important to know how long people have to evacuate from the area when a Flood warning is given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak[`target]:peak[`delta_peak]<3.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs to the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns have been divided into three seperate sections:\n",
    "\n",
    "    -ungaged basin with no perfect forcasts\n",
    "    -gaged basins with no perfect forcasts\n",
    "    -gaged basin with perfect forcasts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monthly Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When forcasts are provided for each model, knowing how the average rainfall will change over differnet window sizes for each month is a feature that can be added to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rain:window_feat[prev_rain;enlist 1_til 15;`ppt`upstr_ppt_1;();(`date`site_no)!\n",
    "         (($;enlist`month;`date);`site_no);`window;!]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is then broken up into monthly values by taking the first day of each month at a given site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "height_data:update date:\"m\"$date from select from all_height where 1=`dd$date\n",
    "rain_data:update date:\"m\"$date from select from all_rain where 1=`dd$date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the stream height and rain data based on site number and month, which is then combined with the basin characteristics and Flood threshold dataset by site number and year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "height_rain:height_data ij`site_no`date xkey rain_data\n",
    "all_monthly_data:(height_rain ij`site_no`year xkey stream_char)ij`site_no`date xkey threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lagged features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For gaged basins, having knowledge of if an area has flooded in the past or not is also an important feature to extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_monthly_data:prev_col[all_monthly_data;enlist 1 12;`target;(enlist `site_no)!enlist `site_no;`lagged]\n",
    "catch_tgts:0!select site_no,no_Flood,date,cs:count date by site_no from all_monthly_data\n",
    "all_monthly_data[`lagged_target_all]:raze{count[x]mavg raze x}each?[catch_tgts;();();`no_Flood]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time_peak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the time to peak dataset the day rain and height data is joined to the FLASH dataset based on the site number and date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_peak_data:((peak ij`date`site_no xkey prev_rain)ij`date`site_no xkey all_height)ij`site_no`year xkey stream_char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prev window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the times from the stream height dataset consistent with the FLASH dataset, the times have to be changed depending on the time zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_zone:raze{dd:x[1];si:x[0];\n",
    "          select `$first site_no,`$first unk from str where date=first dd,si=`$site_no}each \n",
    "          value each 0!select date by distinct site_no from all_peak_data;\n",
    "all_peak_data:all_peak_data ij`site_no xkey time_zone\n",
    "change_zone:{$[y=`EDT;neg[04:00]+x;y=`CDT;neg[05:00]+x;neg[06:00]+x]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_peak_data[`start_time`peak_time]:change_zone''[;all_peak_data`unk]each all_peak_data[`start_time`delta_peak]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous hours before the flood event occurs can show information on how quickly the stream gauge height is moving which can be useful to the model in predicting how long it will take to peak. To do this the 2 days before the event are extracted, and moving averages of different sizes are applied to the stream height dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wind_ht_prev:raze{window_feat[str;enlist 2 4 12 48;`height;((within;`date;(,;(+;(-:;2);x[1]);x[1]))\n",
    "             ;(=;x[0];($;enlist`;`site_no));(within;`datetime;(,;(+;(-:;2);x[2]);x[2])))\n",
    "             ;0b;`wind_prev;?]}each flip all_peak_data[`site_no`date`start_time]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_peak_data:all_peak_data,'wind_ht_prev\n",
    "all_peak_data:del[all_peak_data;`wind_prev_height_2;neg[0w]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For models that use rain forcasts,knowing what the coming days rainfall will be is also useful to add to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rain_pred:raze{window_feat[all_rain;enlist 1_til x;`ppt`upstr_ppt_1;((within;`date;(,;y[1];(+;y[1];x)));\n",
    "          (=;y[0];`site_no));0b;`fut_window;?]}[3]each flip all_peak_data[`site_no`date]\n",
    "all_peak_data:all_peak_data,'rain_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete any rows from the dataset that contains a null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_null:{where not any each null x}\n",
    "cleaned_monthly:all_monthly_data[del_null all_monthly_data]\n",
    "cleaned_peak:   all_peak_data[del_null all_peak_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnd_col:{x where x in y}\n",
    "ungaged_noforcast_basinM: fnd_col[ungaged_noforcast_basin;cols cleaned_monthly]\n",
    "gaged_basinM:             fnd_col[gaged_basin;cols cleaned_monthly]\n",
    "perfect_forcastM:         fnd_col[perfect_forcast;cols cleaned_monthly]\n",
    "ungaged_noforcast_basinP: fnd_col[ungaged_noforcast_basin;cols cleaned_peak]\n",
    "gaged_basinP:             fnd_col[gaged_basin;cols cleaned_peak]\n",
    "perfect_forcastP:         fnd_col[perfect_forcast;cols cleaned_peak]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ungage: `M`P!(ungaged_noforcast_basinM;ungaged_noforcast_basinP)\n",
    "gage:   `M`P!(ungage[`M],gaged_basinM;ungage[`P],gaged_basinP)\n",
    "forcast:`M`P!(gage[`M],perfect_forcastM;gage[`P],perfect_forcastP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dict:{(!). flip(\n",
    "  (`ungaged;flip x[ungage[y]]);\n",
    "  (`gaged;flip x[gage[y]]);\n",
    "  (`forcast;flip x[forcast[y]]))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monthly "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the train and test up by date, so that only past data is contained in the training set. A date was chosen so that 20 percent of the dataset is in the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff:update cutoff:min[date]+floor 0.8*max[date]-min[date] by site_no from cleaned_monthly\n",
    "XtrainMi:select from cutoff where date<cutoff\n",
    "ytrainM:exec target from cutoff where date<cutoff\n",
    "XtestMi:select from cutoff where date>=cutoff\n",
    "ytestM:exec target from cutoff where date>=cutoff\n",
    "\n",
    "XtrainM:split_dict[XtrainMi;`M]\n",
    "XtestM:split_dict[XtestMi;`M]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The time to peak data is seperated so that sites do not appear in both the train and test dataset. To distribution the sum of the target per site is shown below. This distribution is seperated into equeally distributed bins and the train and test split is stratified by this split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 460.8x345.6 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sites:0!select sum target by site_no from cleaned_peak\n",
    "plt[`:hist][sites`target];\n",
    "plt[`:show][];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins:0 5 15 25.0\n",
    "y_binned:bins bin`float$sites`target\n",
    "tts:train_test_split[sites[`site_no];sites[`target];`test_size pykw 0.2; `random_state pykw 607;\n",
    "    `shuffle pykw 1b;`stratify pykw y_binned]`;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_peak[`split]:`TRAIN\n",
    "peak_split:update split:`TEST from cleaned_peak where site_no in `$tts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "XtrainPi:select from peak_split where split=`TRAIN\n",
    "ytrainP:exec target from peak_split where split=`TRAIN\n",
    "XtestPi:select from  peak_split where split=`TEST\n",
    "ytestP:exec target from peak_split where split=`TEST\n",
    "\n",
    "XtrainP:split_dict[XtrainPi;`P]\n",
    "XtestP:split_dict[XtestPi;`P]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosts and Random forests were applied to the train and test datasets and the results were printed as a precision/recall curve. This metric was chosen as getting a balance between precision and recall when predicting floods is pivitol to ensure that all floods are given warnings but also not too many false warnings are given. The inputs to the pr_curve are\n",
    "\n",
    "    -Xtest\n",
    "    -ytest\n",
    "    -dictionary of models that are being used\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_model:{[Xtrain;ytrain;dict]\n",
    " rf_clf:      RandomForestClassifier[`n_estimators pykw dict`rf_n;`random_state pykw 0;`class_weight pykw \n",
    "            (0 1)!(1;dict`rf_wgt)\n",
    "              ][`:fit][Xtrain; ytrain];\n",
    " xgboost_clf: xgboost_clf: XGBClassifier[`n_estimators pykw dict`xgb_n;`learning_rate pykw dict`xgb_lr;\n",
    "            `random_state pykw 0;`scale_pos_weight pykw dict`xgb_wgt;`max_depth pykw dict`xgb_maxd\n",
    "              ][`:fit][np[`:array]Xtrain; ytrain];\n",
    " `random_forest`GBDT!(rf_clf;xgboost_clf)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perfect Forcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict:`rf_n`rf_wgt`xgb_n`xgb_lr`xgb_wgt`xgb_maxd!(100;15;100;0.2;15;7)\n",
    "pr_curve[XtestM`forcast;ytestM;build_model[XtrainM`forcast;ytrainM;dict]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peak Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dict:`rf_n`rf_wgt`rf_maxd`xgb_n`xgb_lr`xgb_wgt`xgb_maxd!(100;1;17;300;0.01;2.5;3)\n",
    "pr_curve[XtestP`forcast;ytestP;build_model[XtrainP`forcast;ytrainP;dict]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaged Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monthly Predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict:`rf_n`rf_wgt`rf_maxd`xgb_n`xgb_lr`xgb_wgt`xgb_maxd!(200;15;8;100;.2;15;7)\n",
    "pr_curve[XtestM`gaged;ytestM;build_model[XtrainM`gaged;ytrainM;dict]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peak Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict:`rf_n`rf_wgt`rf_maxd`xgb_n`xgb_lr`xgb_wgt`xgb_maxd!(100;1;17;360;0.01;1.5;3)\n",
    "pr_curve[XtestP`gaged;ytestP;build_model[XtrainP`gaged;ytrainP;dict]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ungaged Prediction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monthly Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict:`rf_n`rf_wgt`rf_maxd`xgb_n`xgb_lr`xgb_wgt`xgb_maxd!(200;1;8;200;.2;15;7)\n",
    "pr_curve[XtestM`ungaged;ytestM;build_model[XtrainM`ungaged;ytrainM;dict]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peak Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict:`rf_n`rf_wgt`rf_maxd`xgb_n`xgb_lr`xgb_wgt`xgb_maxd!(100;1;17;350;.01;1.5;3)\n",
    "pr_curve[XtestP`ungaged;ytestP;build_model[XtrainP`ungaged;ytrainP;dict]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"The top 15 significant features in predicting the target values for the monthly predicts are:\"\n",
    "string .ml.fresh.significantfeatures[flip forcast[`M]!cleaned_monthly[forcast[`M]];\n",
    " cleaned_monthly`target;.ml.fresh.ksigfeat 15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"The top 15 significant features in predicting the target values for the time-peak are:\"\n",
    "string .ml.fresh.significantfeatures[flip forcast[`P]!cleaned_peak[forcast[`P]];\n",
    " cleaned_peak`target;.ml.fresh.ksigfeat 15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raster Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global Surface Water(GSW) maps the distribution maps the location and temporal distribution of water over the past 3.5 decades globally. This dataset was devoloped in the framework or the Copernicus Programme and was produced by the Landsat satellite. \n",
    "The imagery data contains 5 bands:\n",
    "\n",
    "        -Water Occurence\n",
    "        -Water Occurence Change Intensity\n",
    "        -Water Seasonality\n",
    "        -Annual Water Reoccurance \n",
    "        \n",
    "For the sake of this project we will be looking at 3 bands: Occurence, Seasonality and Reoccurance.\n",
    "\n",
    "The model will be trained using keras VGG16 model using pre-trained weights from imagenet. Imagenet is a database that contains over 14 million images for more than 20,000 classes of objects. Using pretrained weights allows the model to train faster (if pretrained weights are not used, it could take 100's of hours to train) and saves money and effort.\n",
    "\n",
    "The GSW imagery is joined with the the tabular data above based on the site number. Two seperate keras models are built for the imagery and tabular data respectively, which is then concatenated together and passed through another neural net. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG16:          .p.import[`keras.applications.vgg16]`:VGG16\n",
    "pylist:         .p.import[`builtins]`:list\n",
    "Input:          .p.import[`keras.layers]`:Input\n",
    "Dense:          .p.import[`keras.layers]`:Dense\n",
    "Flatten:        .p.import[`keras.layers]`:Flatten\n",
    "Dropout:        .p.import[`keras.layers]`:Dropout\n",
    "Model:          .p.import[`keras]`:Model\n",
    "concatenate:    .p.import[`keras.layers]`:concatenate\n",
    "sequential:     .p.import[`keras.models]`:Sequential\n",
    "dense:          .p.import[`keras.layers]`:Dense\n",
    "normalization:  .p.import[`keras.layers]`:BatchNormalization\n",
    "pylist:         .p.import[`builtins]`:list\n",
    "adam:           .p.import[`keras.optimizers]`:Adam\n",
    "ResNet50:       .p.import[`keras.applications.resnet50]`:ResNet50\n",
    "rio:            .p.import[`rasterio]\n",
    "tuple:          .p.import[`builtins]`:tuple\n",
    "K:              .p.import[`keras]`:backend\n",
    "earlystop:      .p.import[`keras.callbacks]`:EarlyStopping\n",
    "km:             .p.import[`keras_metrics]\n",
    "recall:         km[`:binary_recall]\n",
    "prec:           km[`:binary_precision]\n",
    "\n",
    "K[`:set_image_data_format][`channels_first]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raster:{rio[`:open][x]}each `$\"../../data/raster/\",/:string key hsym `$\"../../data/raster\"\n",
    "descr:{(x`:descriptions)`}first  raster\n",
    "raster_read:{x[`:read][]`}each raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raster:{(x-ml)%((ml:min l)-max l:raze raze `int$raster_read[x])}each 0 3 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raster_tab:flip (`$descr[0 3 4])!raster\n",
    "rastertab:([] site_no:distinct cleaned_monthly`site_no),'raster_tab\n",
    "raster_all:(peak_split ij `site_no xkey rastertab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_raster:Input[`shape pykw pylist (3;256;256)]\n",
    "raster_model:VGG16[`include_top pykw 0b;`input_shape pykw pylist 3 256 256;`weights pykw `imagenet]\n",
    "raster_model:raster_model[input_raster]\n",
    "raster_model_1:Flatten[][raster_model]\n",
    "raster_model_2:Dense[40;`activation pykw `relu;`trainable pykw 0b][raster_model_1]\n",
    "raster_model_2:Dropout[0.2][raster_model_2]\n",
    "raster_model_f:Model[`inputs pykw input_raster;`outputs pykw raster_model_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tab:Input[`shape pykw enlist 53]\n",
    "tab_model:dense[40;`activation pykw `relu][input_tab];\n",
    "tab_model:normalization[][tab_model];\n",
    "tab_model:dense[40;`activation pykw`relu][tab_model];\n",
    "tab_model:normalization[][tab_model];\n",
    "tab_model:dense[40;`activation pykw`relu][tab_model];\n",
    "tab:model:Dropout[0.02]\n",
    "tab_model:normalization[][tab_model];\n",
    "tab_model_f:Model[`inputs pykw input_tab;`outputs pykw tab_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined:concatenate[((raster_model_f[`:output])`;(tab_model_f[`:output])`)];\n",
    "model:Dense[216;`activation pykw`relu][combined];\n",
    "final_m:normalization[][model];\n",
    "final_m:dense[216;`activation pykw`relu][final_m];\n",
    "final_m:dense[216;`activation pykw`relu][final_m];\n",
    "final_m:normalization[][final_m];\n",
    "final_m:dense[216;`activation pykw`relu][final_m];\n",
    "final_m:dense[216;`activation pykw`relu][final_m];\n",
    "final_m:normalization[][final_m];\n",
    "final_m:dense[1;`activation pykw`relu][model];\n",
    "final:Model[`inputs pykw ((raster_model_f[`:input])`;(tab_model_f[`:input])`);`outputs pykw final_m]\n",
    "final[`:compile][`loss pykw`binary_crossentropy;`optimizer pykw `adam;\n",
    "    `metrics pykw pylist pylist (recall[]`;prec[]`)]; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XtrainPi:select from raster_all where split=`TRAIN\n",
    "ytrainP:exec target from raster_all where split=`TRAIN\n",
    "XtestPi:select from  raster_all where split=`TEST\n",
    "ytestP:exec target from raster_all where split=`TEST\n",
    "\n",
    "XtrainP:split_dict[XtrainPi;`P]\n",
    "XtestP:split_dict[XtestPi;`P]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs:(flip XtrainPi[`$descr[0 3 4]];flip .ml.stdscaler [flip XtrainP`ungaged])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs:20\n",
    "batchsz:200\n",
    "es:earlystop[`monitor pykw `val_loss;`mode pykw `min;`verbose pykw 1;`patience pykw 3];\n",
    "final[`:fit][`x pykw inputs;`y pykw ytrainP;`batch_size pykw batchsz;\n",
    "    `verbose pykw 2;`epochs pykw epochs;`validation_split pykw 0.2;\n",
    "    `callbacks pykw enlist es`];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnPred:(raze(final[`:predict](flip XtestPi[`$descr[0 3 4]];flip .ml.stdscaler [flip XtestP`ungaged]))`)>=0.5\n",
    "-1\"\\nConfusion matrix for neural network model:\\n\";\n",
    "(count where ytestP=nnPred)%count[ytestP]\n",
    "nnCM:.ml.classreport[ytestP;nnPred]\n",
    "nnCM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphing Results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmaps:.p.import[`gmaps]\n",
    "display:{x y;}.p.import[`kxpy.kx_backend_inline;`:display]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmaps[`:configure][`api_key pykw \"AIzaSyD7kUIRpebVOP7f_pywIHhO5xPbzY32gRg\"];\n",
    "fig:gmaps[`:figure][];\n",
    "fig[`:add_layer][gmaps[`:heatmap_layer][flip XtestPi[where nnPred=1]`lat`long]];\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above results we can predict with hight accuracy the ............."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Q (kdb+)",
   "language": "q",
   "name": "qpk"
  },
  "language_info": {
   "file_extension": ".q",
   "mimetype": "text/x-q",
   "name": "q",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
